{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ea0a12-5ae9-4740-ae8c-adf236628488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "290535c1-6e6d-48cd-bb34-1d191d616451",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../weights/epoch_92400_sample_23500000.pkl\", \"rb\") as f:\n",
    "    n_annotations, pretrained_model_weights, pretrained_optimizer_weights = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "88031359-70bf-43da-98c3-04d1d96fc5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def max_neg_value(t):\n",
    "    return -torch.finfo(t.dtype).max\n",
    "\n",
    "# helper classes\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "class GlobalLinearSelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_head,\n",
    "        heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, feats, mask = None):\n",
    "        h = self.heads\n",
    "        q, k, v = self.to_qkv(feats).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b n -> b () n ()')\n",
    "            k = k.masked_fill(~mask, -torch.finfo(k.dtype).max)\n",
    "\n",
    "        q = q.softmax(dim = -1)\n",
    "        k = k.softmax(dim = -2)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            v = v.masked_fill(~mask, 0.)\n",
    "\n",
    "        context = einsum('b h n d, b h n e -> b h d e', k, v)\n",
    "        out = einsum('b h d e, b h n d -> b h n e', context, q)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_keys,\n",
    "        dim_out,\n",
    "        heads,\n",
    "        dim_head = 64,\n",
    "        qk_activation = nn.Tanh()\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.qk_activation = qk_activation\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim_keys, inner_dim * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim_out)\n",
    "\n",
    "        self.null_key = nn.Parameter(torch.randn(dim_head))\n",
    "        self.null_value = nn.Parameter(torch.randn(dim_head))\n",
    "\n",
    "    def forward(self, x, context, mask = None, context_mask = None):\n",
    "        b, h, device = x.shape[0], self.heads, x.device\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "\n",
    "        null_k, null_v = map(lambda t: repeat(t, 'd -> b h () d', b = b, h = h), (self.null_key, self.null_value))\n",
    "        k = torch.cat((null_k, k), dim = -2)\n",
    "        v = torch.cat((null_v, v), dim = -2)\n",
    "\n",
    "        q, k = map(lambda t: self.qk_activation(t), (q, k))\n",
    "\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask) or exists(context_mask):\n",
    "            i, j = sim.shape[-2:]\n",
    "\n",
    "            if not exists(mask):\n",
    "                mask = torch.ones(b, i, dtype = torch.bool, device = device)\n",
    "\n",
    "            if exists(context_mask):\n",
    "                context_mask = F.pad(context_mask, (1, 0), value = True)\n",
    "            else:\n",
    "                context_mask = torch.ones(b, j, dtype = torch.bool, device = device)\n",
    "\n",
    "            mask = rearrange(mask, 'b i -> b () i ()') * rearrange(context_mask, 'b j -> b () () j')\n",
    "            sim.masked_fill_(~mask, max_neg_value(sim))\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_global,\n",
    "        narrow_conv_kernel = 9,\n",
    "        wide_conv_kernel = 9,\n",
    "        wide_conv_dilation = 5,\n",
    "        attn_heads = 8,\n",
    "        attn_dim_head = 64,\n",
    "        attn_qk_activation = nn.Tanh(),\n",
    "        local_to_global_attn = False,\n",
    "        local_self_attn = False,\n",
    "        glu_conv = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_self_attn = GlobalLinearSelfAttention(dim = dim, dim_head = attn_dim_head, heads = attn_heads) if local_self_attn else None\n",
    "\n",
    "        conv_mult = 2 if glu_conv else 1\n",
    "\n",
    "        self.narrow_conv = nn.Sequential(\n",
    "            nn.Conv1d(dim, dim * conv_mult, narrow_conv_kernel, padding = narrow_conv_kernel // 2),\n",
    "            nn.GELU() if not glu_conv else nn.GLU(dim = 1)\n",
    "        )\n",
    "\n",
    "        wide_conv_padding = (wide_conv_kernel + (wide_conv_kernel - 1) * (wide_conv_dilation - 1)) // 2\n",
    "\n",
    "        self.wide_conv = nn.Sequential(\n",
    "            nn.Conv1d(dim, dim * conv_mult, wide_conv_kernel, dilation = wide_conv_dilation, padding = wide_conv_padding),\n",
    "            nn.GELU() if not glu_conv else nn.GLU(dim = 1)\n",
    "        )\n",
    "\n",
    "        self.local_to_global_attn = local_to_global_attn\n",
    "\n",
    "        if local_to_global_attn:\n",
    "            self.extract_global_info = CrossAttention(\n",
    "                dim = dim,\n",
    "                dim_keys = dim_global,\n",
    "                dim_out = dim,\n",
    "                heads = attn_heads,\n",
    "                dim_head = attn_dim_head\n",
    "            )\n",
    "        else:\n",
    "            self.extract_global_info = nn.Sequential(\n",
    "                Reduce('b n d -> b d', 'mean'),\n",
    "                nn.Linear(dim_global, dim),\n",
    "                nn.GELU(),\n",
    "                Rearrange('b d -> b () d')\n",
    "            )\n",
    "\n",
    "        self.local_norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.local_feedforward = nn.Sequential(\n",
    "            Residual(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.GELU(),\n",
    "            )),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "        self.global_attend_local = CrossAttention(dim = dim_global, dim_out = dim_global, dim_keys = dim, heads = attn_heads, dim_head = attn_dim_head, qk_activation = attn_qk_activation)\n",
    "\n",
    "        self.global_dense = nn.Sequential(\n",
    "            nn.Linear(dim_global, dim_global),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.global_norm = nn.LayerNorm(dim_global)\n",
    "\n",
    "        self.global_feedforward = nn.Sequential(\n",
    "            Residual(nn.Sequential(\n",
    "                nn.Linear(dim_global, dim_global),\n",
    "                nn.GELU()\n",
    "            )),\n",
    "            nn.LayerNorm(dim_global),\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens, annotation, mask = None):\n",
    "        if self.local_to_global_attn:\n",
    "            global_info = self.extract_global_info(tokens, annotation, mask = mask)\n",
    "        else:\n",
    "            global_info = self.extract_global_info(annotation)\n",
    "\n",
    "        # process local (protein sequence)\n",
    "\n",
    "        global_linear_attn = self.seq_self_attn(tokens) if exists(self.seq_self_attn) else 0\n",
    "\n",
    "        conv_input = rearrange(tokens, 'b n d -> b d n')\n",
    "\n",
    "        if exists(mask):\n",
    "            conv_input_mask = rearrange(mask, 'b n -> b () n')\n",
    "            conv_input = conv_input.masked_fill(~conv_input_mask, 0.)\n",
    "\n",
    "        narrow_out = self.narrow_conv(conv_input)\n",
    "        narrow_out = rearrange(narrow_out, 'b d n -> b n d')\n",
    "        wide_out = self.wide_conv(conv_input)\n",
    "        wide_out = rearrange(wide_out, 'b d n -> b n d')\n",
    "\n",
    "        tokens = tokens + narrow_out + wide_out + global_info + global_linear_attn\n",
    "        tokens = self.local_norm(tokens)\n",
    "\n",
    "        tokens = self.local_feedforward(tokens)\n",
    "\n",
    "        # process global (annotations)\n",
    "\n",
    "        annotation = self.global_attend_local(annotation, tokens, context_mask = mask)\n",
    "        annotation = self.global_dense(annotation)\n",
    "        annotation = self.global_norm(annotation)\n",
    "        annotation = self.global_feedforward(annotation)\n",
    "\n",
    "        return tokens, annotation\n",
    "\n",
    "# main model\n",
    "\n",
    "class ProteinBERT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        num_tokens = 26,\n",
    "        num_annotation = 8943,\n",
    "        dim = 512,\n",
    "        dim_global = 256,\n",
    "        depth = 6,\n",
    "        narrow_conv_kernel = 9,\n",
    "        wide_conv_kernel = 9,\n",
    "        wide_conv_dilation = 5,\n",
    "        attn_heads = 8,\n",
    "        attn_dim_head = 64,\n",
    "        attn_qk_activation = nn.Tanh(),\n",
    "        local_to_global_attn = False,\n",
    "        local_self_attn = False,\n",
    "        num_global_tokens = 1,\n",
    "        glu_conv = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.token_emb = nn.Embedding(num_tokens, dim)\n",
    "\n",
    "        self.num_global_tokens = num_global_tokens\n",
    "        self.to_global_emb = nn.Linear(num_annotation, num_global_tokens * dim_global)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            Layer(\n",
    "                dim = dim,\n",
    "                dim_global = dim_global,\n",
    "                narrow_conv_kernel = narrow_conv_kernel,\n",
    "                wide_conv_dilation = wide_conv_dilation,\n",
    "                wide_conv_kernel = wide_conv_kernel,\n",
    "                attn_qk_activation = attn_qk_activation,\n",
    "                local_to_global_attn = local_to_global_attn,\n",
    "                local_self_attn = local_self_attn,\n",
    "                glu_conv = glu_conv\n",
    "            )\n",
    "            for layer in range(depth)\n",
    "        ])\n",
    "        # print(self.layers)\n",
    "\n",
    "        self.to_token_logits = nn.Linear(dim, num_tokens)\n",
    "\n",
    "        self.to_annotation_logits = nn.Sequential(\n",
    "            Reduce('b n d -> b d', 'mean'),\n",
    "            nn.Linear(dim_global, num_annotation)\n",
    "        )\n",
    "\n",
    "    def forward(self, seq, annotation, mask = None):\n",
    "        tokens = self.token_emb(seq)\n",
    "\n",
    "        annotation = self.to_global_emb(annotation)\n",
    "        annotation = rearrange(annotation, 'b (n d) -> b n d', n = self.num_global_tokens)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            tokens, annotation = layer(tokens, annotation, mask = mask)\n",
    "\n",
    "        tokens = self.to_token_logits(tokens)\n",
    "        annotation = self.to_annotation_logits(annotation)\n",
    "        return tokens, annotation\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b2d1b14a-9cee-49a9-b3d8-f44a4b311cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5929, -0.9098,  0.9188,  ..., -0.1676, -0.7180, -0.4313],\n",
      "         [ 1.1562,  0.2398, -0.0485,  ..., -0.6763, -0.7170, -0.8447],\n",
      "         [ 0.2704,  0.0859, -0.0987,  ..., -0.8367, -0.3104, -0.2539],\n",
      "         ...,\n",
      "         [ 0.5719, -0.4015, -0.4394,  ...,  0.8981, -0.8608, -0.4114],\n",
      "         [ 0.1337, -0.0078, -0.0260,  ..., -0.2980,  0.3003, -0.5458],\n",
      "         [ 0.8318, -0.6286,  0.4875,  ..., -0.1456, -0.1285, -0.3256]],\n",
      "\n",
      "        [[ 1.0427,  0.2485, -0.6645,  ..., -0.4918,  0.3169,  0.1762],\n",
      "         [ 0.5184,  0.0329,  0.8701,  ..., -0.2437, -0.1188, -0.2436],\n",
      "         [ 0.2250, -0.6192,  1.0777,  ..., -0.6017,  0.6816, -0.3975],\n",
      "         ...,\n",
      "         [ 0.5297,  0.4673, -0.2527,  ..., -0.3983, -0.1364, -0.4662],\n",
      "         [ 0.8680,  0.3956, -0.3994,  ..., -0.5084, -0.1851, -0.5838],\n",
      "         [-0.3677,  0.2939, -0.2362,  ..., -0.5652, -0.1340, -1.0860]]],\n",
      "       grad_fn=<ViewBackward0>) tensor([[ 0.4354, -0.5147, -0.9464,  ...,  0.1424, -0.7635,  0.4639],\n",
      "        [ 0.4558, -0.4097, -0.7130,  ...,  0.1369, -0.7431,  0.4566]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = ProteinBERT(\n",
    "    num_tokens = 21,\n",
    "    num_annotation = 8943,\n",
    "    dim = 512,\n",
    "    dim_global = 256,\n",
    "    depth = 6,\n",
    "    narrow_conv_kernel = 9,\n",
    "    wide_conv_kernel = 9,\n",
    "    wide_conv_dilation = 5,\n",
    "    attn_heads = 8,\n",
    "    attn_dim_head = 64\n",
    ")\n",
    "\n",
    "seq = torch.randint(0, 21, (2, 2048))\n",
    "mask = torch.ones(2, 2048).bool()\n",
    "annotation = torch.randint(0, 1, (2, 8943)).float()\n",
    "\n",
    "seq_logits, annotation_logits = model(seq, annotation, mask = mask) # (2, 2048, 21), (2, 8943)\n",
    "print(seq_logits, annotation_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6c27a49b-e769-4e4e-9e47-bd2c084a34c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProteinBERT(\n",
      "  (token_emb): Embedding(26, 128)\n",
      "  (to_global_emb): Linear(in_features=8943, out_features=512, bias=True)\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x Layer(\n",
      "      (narrow_conv): Sequential(\n",
      "        (0): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "        (1): GELU(approximate='none')\n",
      "      )\n",
      "      (wide_conv): Sequential(\n",
      "        (0): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(20,), dilation=(5,))\n",
      "        (1): GELU(approximate='none')\n",
      "      )\n",
      "      (extract_global_info): Sequential(\n",
      "        (0): Reduce('b n d -> b d', 'mean')\n",
      "        (1): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Rearrange('b d -> b () d')\n",
      "      )\n",
      "      (local_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (local_feedforward): Sequential(\n",
      "        (0): Residual(\n",
      "          (fn): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (global_attend_local): CrossAttention(\n",
      "        (qk_activation): Tanh()\n",
      "        (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (to_kv): Linear(in_features=128, out_features=1024, bias=False)\n",
      "        (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (global_dense): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "      )\n",
      "      (global_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (global_feedforward): Sequential(\n",
      "        (0): Residual(\n",
      "          (fn): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (to_token_logits): Linear(in_features=128, out_features=26, bias=True)\n",
      "  (to_annotation_logits): Sequential(\n",
      "    (0): Reduce('b n d -> b d', 'mean')\n",
      "    (1): Linear(in_features=512, out_features=8943, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = ProteinBERT(\n",
    "    num_tokens = 26,\n",
    "    num_annotation = 8943,\n",
    "    dim = 128,\n",
    "    dim_global = 512,\n",
    "    depth = 6,\n",
    "    narrow_conv_kernel = 9,\n",
    "    wide_conv_kernel = 9,\n",
    "    wide_conv_dilation = 5,\n",
    "    attn_heads = 4,\n",
    "    attn_dim_head = 64\n",
    ")\n",
    "print(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2bf809c5-f21f-4ed9-ba90-f0243a5f79de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 8943)          weight\n",
      "(512,)               bias\n",
      "(26, 128)            weight\n",
      "---\n",
      "0    (128, 128, 9)        narrow_conv.0.weight\n",
      "1    (128,)               narrow_conv.0.bias\n",
      "2    (128, 128, 9)        wide_conv.0.weight\n",
      "3    (128,)               wide_conv.0.bias\n",
      "4    (128, 512)           extract_global_info.1.weight\n",
      "5    (128,)               extract_global_info.1.bias\n",
      "6    (128,)               local_norm.weight\n",
      "7    (128,)               local_norm.bias\n",
      "8    (128, 128)           local_feedforward.0.fn.0.weight\n",
      "9    (128,)               local_feedforward.0.fn.0.bias\n",
      "10   (128,)               local_feedforward.1.weight\n",
      "11   (128,)               local_feedforward.1.bias\n",
      "12   (64,)                global_attend_local.null_key\n",
      "13   (64,)                global_attend_local.null_value\n",
      "14   (512, 512)           global_attend_local.to_q.weight\n",
      "15   (1024, 128)          global_attend_local.to_kv.weight\n",
      "16   (512, 512)           global_attend_local.to_out.weight\n",
      "17   (512,)               global_attend_local.to_out.bias\n",
      "18   (512, 512)           global_dense.0.weight\n",
      "19   (512,)               global_dense.0.bias\n",
      "20   (512,)               global_norm.weight\n",
      "21   (512,)               global_norm.bias\n",
      "22   (512, 512)           global_feedforward.0.fn.0.weight\n",
      "23   (512,)               global_feedforward.0.fn.0.bias\n",
      "24   (512,)               global_feedforward.1.weight\n",
      "25   (512,)               global_feedforward.1.bias\n",
      "---\n",
      "0    (128, 128, 9)        narrow_conv.0.weight\n",
      "1    (128,)               narrow_conv.0.bias\n",
      "2    (128, 128, 9)        wide_conv.0.weight\n",
      "3    (128,)               wide_conv.0.bias\n",
      "4    (128, 512)           extract_global_info.1.weight\n",
      "5    (128,)               extract_global_info.1.bias\n",
      "6    (128,)               local_norm.weight\n",
      "7    (128,)               local_norm.bias\n",
      "8    (128, 128)           local_feedforward.0.fn.0.weight\n",
      "9    (128,)               local_feedforward.0.fn.0.bias\n",
      "10   (128,)               local_feedforward.1.weight\n",
      "11   (128,)               local_feedforward.1.bias\n",
      "12   (64,)                global_attend_local.null_key\n",
      "13   (64,)                global_attend_local.null_value\n",
      "14   (512, 512)           global_attend_local.to_q.weight\n",
      "15   (1024, 128)          global_attend_local.to_kv.weight\n",
      "16   (512, 512)           global_attend_local.to_out.weight\n",
      "17   (512,)               global_attend_local.to_out.bias\n",
      "18   (512, 512)           global_dense.0.weight\n",
      "19   (512,)               global_dense.0.bias\n",
      "20   (512,)               global_norm.weight\n",
      "21   (512,)               global_norm.bias\n",
      "22   (512, 512)           global_feedforward.0.fn.0.weight\n",
      "23   (512,)               global_feedforward.0.fn.0.bias\n",
      "24   (512,)               global_feedforward.1.weight\n",
      "25   (512,)               global_feedforward.1.bias\n",
      "---\n",
      "0    (128, 128, 9)        narrow_conv.0.weight\n",
      "1    (128,)               narrow_conv.0.bias\n",
      "2    (128, 128, 9)        wide_conv.0.weight\n",
      "3    (128,)               wide_conv.0.bias\n",
      "4    (128, 512)           extract_global_info.1.weight\n",
      "5    (128,)               extract_global_info.1.bias\n",
      "6    (128,)               local_norm.weight\n",
      "7    (128,)               local_norm.bias\n",
      "8    (128, 128)           local_feedforward.0.fn.0.weight\n",
      "9    (128,)               local_feedforward.0.fn.0.bias\n",
      "10   (128,)               local_feedforward.1.weight\n",
      "11   (128,)               local_feedforward.1.bias\n",
      "12   (64,)                global_attend_local.null_key\n",
      "13   (64,)                global_attend_local.null_value\n",
      "14   (512, 512)           global_attend_local.to_q.weight\n",
      "15   (1024, 128)          global_attend_local.to_kv.weight\n",
      "16   (512, 512)           global_attend_local.to_out.weight\n",
      "17   (512,)               global_attend_local.to_out.bias\n",
      "18   (512, 512)           global_dense.0.weight\n",
      "19   (512,)               global_dense.0.bias\n",
      "20   (512,)               global_norm.weight\n",
      "21   (512,)               global_norm.bias\n",
      "22   (512, 512)           global_feedforward.0.fn.0.weight\n",
      "23   (512,)               global_feedforward.0.fn.0.bias\n",
      "24   (512,)               global_feedforward.1.weight\n",
      "25   (512,)               global_feedforward.1.bias\n",
      "---\n",
      "0    (128, 128, 9)        narrow_conv.0.weight\n",
      "1    (128,)               narrow_conv.0.bias\n",
      "2    (128, 128, 9)        wide_conv.0.weight\n",
      "3    (128,)               wide_conv.0.bias\n",
      "4    (128, 512)           extract_global_info.1.weight\n",
      "5    (128,)               extract_global_info.1.bias\n",
      "6    (128,)               local_norm.weight\n",
      "7    (128,)               local_norm.bias\n",
      "8    (128, 128)           local_feedforward.0.fn.0.weight\n",
      "9    (128,)               local_feedforward.0.fn.0.bias\n",
      "10   (128,)               local_feedforward.1.weight\n",
      "11   (128,)               local_feedforward.1.bias\n",
      "12   (64,)                global_attend_local.null_key\n",
      "13   (64,)                global_attend_local.null_value\n",
      "14   (512, 512)           global_attend_local.to_q.weight\n",
      "15   (1024, 128)          global_attend_local.to_kv.weight\n",
      "16   (512, 512)           global_attend_local.to_out.weight\n",
      "17   (512,)               global_attend_local.to_out.bias\n",
      "18   (512, 512)           global_dense.0.weight\n",
      "19   (512,)               global_dense.0.bias\n",
      "20   (512,)               global_norm.weight\n",
      "21   (512,)               global_norm.bias\n",
      "22   (512, 512)           global_feedforward.0.fn.0.weight\n",
      "23   (512,)               global_feedforward.0.fn.0.bias\n",
      "24   (512,)               global_feedforward.1.weight\n",
      "25   (512,)               global_feedforward.1.bias\n",
      "---\n",
      "0    (128, 128, 9)        narrow_conv.0.weight\n",
      "1    (128,)               narrow_conv.0.bias\n",
      "2    (128, 128, 9)        wide_conv.0.weight\n",
      "3    (128,)               wide_conv.0.bias\n",
      "4    (128, 512)           extract_global_info.1.weight\n",
      "5    (128,)               extract_global_info.1.bias\n",
      "6    (128,)               local_norm.weight\n",
      "7    (128,)               local_norm.bias\n",
      "8    (128, 128)           local_feedforward.0.fn.0.weight\n",
      "9    (128,)               local_feedforward.0.fn.0.bias\n",
      "10   (128,)               local_feedforward.1.weight\n",
      "11   (128,)               local_feedforward.1.bias\n",
      "12   (64,)                global_attend_local.null_key\n",
      "13   (64,)                global_attend_local.null_value\n",
      "14   (512, 512)           global_attend_local.to_q.weight\n",
      "15   (1024, 128)          global_attend_local.to_kv.weight\n",
      "16   (512, 512)           global_attend_local.to_out.weight\n",
      "17   (512,)               global_attend_local.to_out.bias\n",
      "18   (512, 512)           global_dense.0.weight\n",
      "19   (512,)               global_dense.0.bias\n",
      "20   (512,)               global_norm.weight\n",
      "21   (512,)               global_norm.bias\n",
      "22   (512, 512)           global_feedforward.0.fn.0.weight\n",
      "23   (512,)               global_feedforward.0.fn.0.bias\n",
      "24   (512,)               global_feedforward.1.weight\n",
      "25   (512,)               global_feedforward.1.bias\n",
      "---\n",
      "0    (128, 128, 9)        narrow_conv.0.weight\n",
      "1    (128,)               narrow_conv.0.bias\n",
      "2    (128, 128, 9)        wide_conv.0.weight\n",
      "3    (128,)               wide_conv.0.bias\n",
      "4    (128, 512)           extract_global_info.1.weight\n",
      "5    (128,)               extract_global_info.1.bias\n",
      "6    (128,)               local_norm.weight\n",
      "7    (128,)               local_norm.bias\n",
      "8    (128, 128)           local_feedforward.0.fn.0.weight\n",
      "9    (128,)               local_feedforward.0.fn.0.bias\n",
      "10   (128,)               local_feedforward.1.weight\n",
      "11   (128,)               local_feedforward.1.bias\n",
      "12   (64,)                global_attend_local.null_key\n",
      "13   (64,)                global_attend_local.null_value\n",
      "14   (512, 512)           global_attend_local.to_q.weight\n",
      "15   (1024, 128)          global_attend_local.to_kv.weight\n",
      "16   (512, 512)           global_attend_local.to_out.weight\n",
      "17   (512,)               global_attend_local.to_out.bias\n",
      "18   (512, 512)           global_dense.0.weight\n",
      "19   (512,)               global_dense.0.bias\n",
      "20   (512,)               global_norm.weight\n",
      "21   (512,)               global_norm.bias\n",
      "22   (512, 512)           global_feedforward.0.fn.0.weight\n",
      "23   (512,)               global_feedforward.0.fn.0.bias\n",
      "24   (512,)               global_feedforward.1.weight\n",
      "25   (512,)               global_feedforward.1.bias\n",
      "---\n",
      "(26, 128)            weight\n",
      "(26,)                bias\n",
      "(8943, 512)          1.weight\n",
      "(8943,)              1.bias\n",
      "163\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for name, param in pretrained_model.to_global_emb.named_parameters():\n",
    "    print(f\"{str(tuple(param.shape)):<20} {name}\")\n",
    "    count += 1\n",
    "    \n",
    "for name, param in pretrained_model.token_emb.named_parameters():\n",
    "    print(f\"{str(tuple(param.shape)):<20} {name}\")\n",
    "    count += 1\n",
    "\n",
    "print(\"---\")\n",
    "for layer in pretrained_model.layers:\n",
    "    for idx, (name, param) in enumerate(layer.named_parameters()):\n",
    "        print(f\"{idx:<4} {str(tuple(param.shape)):<20} {name}\")\n",
    "        count += 1\n",
    "    print(\"---\")\n",
    "\n",
    "for name, param in pretrained_model.to_token_logits.named_parameters():\n",
    "    print(f\"{str(tuple(param.shape)):<20} {name}\")\n",
    "    count += 1\n",
    "\n",
    "for name, param in pretrained_model.to_annotation_logits.named_parameters():\n",
    "    print(f\"{str(tuple(param.shape)):<20} {name}\")\n",
    "    count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdbcf753-f6d6-430b-8de1-2a65b7c872c6",
   "metadata": {},
   "source": [
    "0 (8943, 512)\n",
    "1 (512,)\n",
    "2 (26, 128)\n",
    "---\n",
    "3 (512, 128)      extract_global_info.1.weight\n",
    "4 (128,)          extract_global_info.1.bias\n",
    "\n",
    "5 (9, 128, 128)   narrow_conv.0.weight\n",
    "6 (128,)          narrow_conv.0.bias\n",
    "\n",
    "7 (9, 128, 128)   wide_conv.0.weight\n",
    "8 (128,)          wide_conv.0.bias\n",
    "\n",
    "9 (128,)          local_norm.weight\n",
    "10 (128,)         local_norm.bias\n",
    "\n",
    "11 (128, 128)     local_feedforward.0.fn.0.weight\n",
    "12 (128,)         local_feedforward.0.fn.0.bias\n",
    "13 (128,)         local_feedforward.1.weight\n",
    "14 (128,)         local_feedforward.1.bias\n",
    "\n",
    "15 (512, 512)     global_dense.0.weight\n",
    "16 (512,)         global_dense.0.bias\n",
    "\n",
    "<-- pytorch\n",
    "12 (64,)          global_attend_local.null_key\n",
    "13 (64,)          global_attend_local.null_value\n",
    "14 (512, 512)     global_attend_local.to_q.weight\n",
    "15 (1024, 128)    global_attend_local.to_kv.weight\n",
    "16 (512, 512)     global_attend_local.to_out.weight\n",
    "17 (512,)         global_attend_local.to_out.bias\n",
    "===\n",
    "17 (4, 512, 64)   global_attend_local  (Wq) (n_heads, d_global_input, d_key)\n",
    "18 (4, 128, 64)                        (Wk) (n_heads, d_seq_input, d_key)\n",
    "19 (4, 128, 128)                       (Wv) (n_heads, d_seq_input, d_value)\n",
    "--> keras\n",
    "\n",
    "20 (512,)         global_norm.weight\n",
    "21 (512,)         global_norm.bias\n",
    "\n",
    "22 (512, 512)     global_feedforward.0.fn.0.weight\n",
    "23 (512,)         global_feedforward.0.fn.0.bias\n",
    "\n",
    "24 (512,)         global_feedforward.1.weight\n",
    "25 (512,)         global_feedforward.1.bias\n",
    "... repeat 5x ...\n",
    "---\n",
    "141 (128, 26)\n",
    "142 (26,)\n",
    "143 (512, 8943)\n",
    "144 (8943,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "29444b16-1624-43c2-9312-1b3053f9a1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (8943, 512)\n",
      "1 (512,)\n",
      "2 (26, 128)\n",
      "---\n",
      "3 (512, 128)\n",
      "4 (128,)\n",
      "5 (9, 128, 128)\n",
      "6 (128,)\n",
      "7 (9, 128, 128)\n",
      "8 (128,)\n",
      "9 (128,)\n",
      "10 (128,)\n",
      "11 (128, 128)\n",
      "12 (128,)\n",
      "13 (128,)\n",
      "14 (128,)\n",
      "15 (512, 512)\n",
      "16 (512,)\n",
      "17 (4, 512, 64)\n",
      "18 (4, 128, 64)\n",
      "19 (4, 128, 128)\n",
      "20 (512,)\n",
      "21 (512,)\n",
      "22 (512, 512)\n",
      "23 (512,)\n",
      "24 (512,)\n",
      "25 (512,)\n",
      "---\n",
      "26 (512, 128)\n",
      "27 (128,)\n",
      "28 (9, 128, 128)\n",
      "29 (128,)\n",
      "30 (9, 128, 128)\n",
      "31 (128,)\n",
      "32 (128,)\n",
      "33 (128,)\n",
      "34 (128, 128)\n",
      "35 (128,)\n",
      "36 (128,)\n",
      "37 (128,)\n",
      "38 (512, 512)\n",
      "39 (512,)\n",
      "40 (4, 512, 64)\n",
      "41 (4, 128, 64)\n",
      "42 (4, 128, 128)\n",
      "43 (512,)\n",
      "44 (512,)\n",
      "45 (512, 512)\n",
      "46 (512,)\n",
      "47 (512,)\n",
      "48 (512,)\n",
      "---\n",
      "49 (512, 128)\n",
      "50 (128,)\n",
      "51 (9, 128, 128)\n",
      "52 (128,)\n",
      "53 (9, 128, 128)\n",
      "54 (128,)\n",
      "55 (128,)\n",
      "56 (128,)\n",
      "57 (128, 128)\n",
      "58 (128,)\n",
      "59 (128,)\n",
      "60 (128,)\n",
      "61 (512, 512)\n",
      "62 (512,)\n",
      "63 (4, 512, 64)\n",
      "64 (4, 128, 64)\n",
      "65 (4, 128, 128)\n",
      "66 (512,)\n",
      "67 (512,)\n",
      "68 (512, 512)\n",
      "69 (512,)\n",
      "70 (512,)\n",
      "71 (512,)\n",
      "---\n",
      "72 (512, 128)\n",
      "73 (128,)\n",
      "74 (9, 128, 128)\n",
      "75 (128,)\n",
      "76 (9, 128, 128)\n",
      "77 (128,)\n",
      "78 (128,)\n",
      "79 (128,)\n",
      "80 (128, 128)\n",
      "81 (128,)\n",
      "82 (128,)\n",
      "83 (128,)\n",
      "84 (512, 512)\n",
      "85 (512,)\n",
      "86 (4, 512, 64)\n",
      "87 (4, 128, 64)\n",
      "88 (4, 128, 128)\n",
      "89 (512,)\n",
      "90 (512,)\n",
      "91 (512, 512)\n",
      "92 (512,)\n",
      "93 (512,)\n",
      "94 (512,)\n",
      "---\n",
      "95 (512, 128)\n",
      "96 (128,)\n",
      "97 (9, 128, 128)\n",
      "98 (128,)\n",
      "99 (9, 128, 128)\n",
      "100 (128,)\n",
      "101 (128,)\n",
      "102 (128,)\n",
      "103 (128, 128)\n",
      "104 (128,)\n",
      "105 (128,)\n",
      "106 (128,)\n",
      "107 (512, 512)\n",
      "108 (512,)\n",
      "109 (4, 512, 64)\n",
      "110 (4, 128, 64)\n",
      "111 (4, 128, 128)\n",
      "112 (512,)\n",
      "113 (512,)\n",
      "114 (512, 512)\n",
      "115 (512,)\n",
      "116 (512,)\n",
      "117 (512,)\n",
      "---\n",
      "118 (512, 128)\n",
      "119 (128,)\n",
      "120 (9, 128, 128)\n",
      "121 (128,)\n",
      "122 (9, 128, 128)\n",
      "123 (128,)\n",
      "124 (128,)\n",
      "125 (128,)\n",
      "126 (128, 128)\n",
      "127 (128,)\n",
      "128 (128,)\n",
      "129 (128,)\n",
      "130 (512, 512)\n",
      "131 (512,)\n",
      "132 (4, 512, 64)\n",
      "133 (4, 128, 64)\n",
      "134 (4, 128, 128)\n",
      "135 (512,)\n",
      "136 (512,)\n",
      "137 (512, 512)\n",
      "138 (512,)\n",
      "139 (512,)\n",
      "140 (512,)\n",
      "---\n",
      "141 (128, 26)\n",
      "142 (26,)\n",
      "143 (512, 8943)\n",
      "144 (8943,)\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(pretrained_model_weights):\n",
    "\n",
    "    print(i, w.shape) # 23 weight objects per transformer layer in keras model?\n",
    "\n",
    "    if i == 2:\n",
    "        print(\"---\")\n",
    "    if i > 3 and (i - 2) % 23 == 0:\n",
    "        print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
