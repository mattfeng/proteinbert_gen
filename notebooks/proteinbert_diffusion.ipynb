{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5545e4a-c6e1-44cd-934f-0ac73c87b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "165a624c-6244-4a16-87d1-282e77237988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import abc\n",
    "import numpy as np\n",
    "from typing import Any, List, Optional, Sequence, Union\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from proteinbert_gen.dataset import sprot_train\n",
    "import proteinbert_gen.constants as consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4703d96-c7bf-4c70-bc0a-6d1e7588ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(sprot_train, batch_size=32)\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7909f5a-f0e7-46ed-adb5-18559d71803b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d563732-4fd6-4cae-8da8-5a43d1953f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54fbdf05-789f-4a5d-a9b4-8d12e7df719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_with_probs(p=None, q=None, epsilon=1e-20):\n",
    "    \"\"\"Compute the KL between two categorical distributions from their probabilities.\n",
    "    Args:\n",
    "    p: [..., dim] array with probs for the first distribution.\n",
    "    q: [..., dim] array with probs for the second distribution.\n",
    "    epsilon: a small float to normalize probabilities with.\n",
    "    Returns:\n",
    "    an array of KL divergence terms taken over the last axis.\n",
    "    \"\"\"\n",
    "    kl = (p * (torch.log(p + epsilon) - torch.log(q + epsilon))).sum(-1)\n",
    "    ## KL divergence should be positive, this helps with numerical stability\n",
    "    loss = torch.nn.functional.relu(kl)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def cross_entropy_with_probs(probs, targets, epsilon=1e-20):\n",
    "    \"\"\"Compute cross entropy for a given distribution and targets.\n",
    "    Cross entropy is taken over the last axis. Remaining axes are unchanged.\n",
    "    Args:\n",
    "    probs: [..., length, num_classes] float array.\n",
    "    targets: categorical targets [..., length] int array.\n",
    "    label_smoothing: label smoothing constant, used to determine the on and off\n",
    "     values.\n",
    "    epsilon: small noise to add to probs when converting to log space.\n",
    "    Returns:\n",
    "    Array with loss taken over the last axis.\n",
    "    \"\"\"\n",
    "    assert probs.size()[:-1] == targets.size(), \"Logits shape must agree with targets, except in the last dimension.\"\n",
    "\n",
    "    # vocab_size = probs.size(-1)\n",
    "\n",
    "    # soft_targets = torch.nn.functional.one_hot(targets, vocab_size)\n",
    "\n",
    "    probs = torch.nn.functional.relu(probs)  # help with numerical stability\n",
    "    loss = -(torch.log(probs + epsilon).gather(-1, targets.unsqueeze(-1))).squeeze(-1)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(\n",
    "    logits: torch.Tensor,\n",
    "    top_k: int = 0,\n",
    "    top_p: float = 1.0,\n",
    "    filter_value: float = -float(\"Inf\"),\n",
    "    min_tokens_to_keep: int = 1,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "    Args:\n",
    "        logits: logits distribution shape (batch size, vocabulary size)\n",
    "        if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "        if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        Make sure we keep at least min_tokens_to_keep per batch example in the output\n",
    "    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    if top_k > 0:\n",
    "        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        if min_tokens_to_keep > 1:\n",
    "            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
    "            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "def fori_loop(lower, upper, body_fun, init_val):\n",
    "    val = init_val\n",
    "    for i in range(lower, upper):\n",
    "        val = body_fun(i, val)\n",
    "    return val\n",
    "\n",
    "\n",
    "def scan(f, init, xs, length=None):\n",
    "    if xs is None:\n",
    "        xs = [None] * length\n",
    "    carry = init\n",
    "    ys = []\n",
    "    for x in xs:\n",
    "        carry, y = f(carry, x)\n",
    "        ys.append(y)\n",
    "\n",
    "    return carry, torch.stack(ys) if ys[0] is not None else None\n",
    "\n",
    "\n",
    "def word_frequency(path, basic_freq=.5):\n",
    "    freq = torch.load(path)\n",
    "    return basic_freq + (1 - basic_freq) * freq / freq.mean()\n",
    "\n",
    "def min_max_norm(t, dim):\n",
    "    return ((t - t.min(dim=dim, keepdims=True).values) / (t.max(dim=dim, keepdims=True).values - t.min(dim=dim, keepdims=True).values)) * 2 - 1\n",
    "\n",
    "\n",
    "\n",
    "class DiffusionSchedule:\n",
    "    \"\"\"A wrapper around a simple schedule function.\"\"\"\n",
    "\n",
    "    def __init__(self, schedule_fn, num_steps, is_constant=False):\n",
    "        self._schedule_fn = schedule_fn\n",
    "        self.num_steps = num_steps\n",
    "        self.is_constant = is_constant\n",
    "\n",
    "    def __call__(self, step):\n",
    "        return self._schedule_fn(step)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"DiffusionSchedule(steps: {self.num_steps}, is_constant: {self.is_constant})\"\n",
    "\n",
    "\n",
    "class DiscreteDiffusionBase(abc.ABC):\n",
    "    num_steps: int\n",
    "    dim: int\n",
    "    tokenizer: Any\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def stationary_probs(self, shape):\n",
    "        \"\"\"Returns probs for the stationary distribution.\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def sample_stationary(self, shape):\n",
    "        \"\"\"Draws a sample from the stationary distribution (q(x_T)).\"\"\"\n",
    "\n",
    "    def sample_t(self, size=(1,)):\n",
    "        \"\"\"Samples batches of time steps to use.\"\"\"\n",
    "        return torch.randint(low=0, high=self.num_steps, size=size, device=self.device)\n",
    "\n",
    "    def supports_efficient_get(self):\n",
    "        \"\"\"Returns true if get() is implemented/efficient.\"\"\"\n",
    "        return False\n",
    "\n",
    "    def supports_efficient_inference(self):\n",
    "        \"\"\"Returns true if custom_product_fn is implemented.\n",
    "        The ontology of efficient_get and efficient_inference is this:\n",
    "          * if efficient_inference is enabled, it is used to return q(x_t | x_0)\n",
    "            without computing expensive products.\n",
    "          * if efficient_get is enabled, get(...) is used to get the posterior of\n",
    "            q(x_{t-1} | x_t, x_0). If not, get_q_given_q0 is called to get\n",
    "            q(x_{t+1} | x_0), and qt_reverse is called to get the q(x_{t+1} | x_0).\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_qt_given_q0(self,\n",
    "                        q0,\n",
    "                        t,\n",
    "                        return_logits=False,\n",
    "                        make_one_hot=False,\n",
    "                        word_freq_logits=None,\n",
    "                        epsilon=1e-20):\n",
    "        \"\"\"Get q(x_t), the n-step posterior.\n",
    "        For example, for t = 0, it returns q0 unchanged.\n",
    "        Args:\n",
    "          q0: an array of floats specifying a distribution over p(x_0).\n",
    "          t: t in q(x_t | x_0).\n",
    "          return_logits: if True, return the output logits\n",
    "          make_one_hot: if True, will convert q0 to floats if needed.\n",
    "          epsilon: a small number to normalize logits conversion with, if needed.\n",
    "        Returns:\n",
    "          q(x_t | x_0).\n",
    "        \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def sample_and_compute_posterior_q(self,\n",
    "                                       x_0,\n",
    "                                       t,\n",
    "                                       samples=None,\n",
    "                                       transition_probs=None,\n",
    "                                       return_logits=True,\n",
    "                                       return_transition_probs=False,\n",
    "                                       transition_probs_in_logits=True,\n",
    "                                       make_one_hot=True,\n",
    "                                       epsilon=1e-20,\n",
    "                                       step_size=1,\n",
    "                                       word_freq_logits=None,\n",
    "                                       ):\n",
    "        \"\"\"Samples from q(x_{t+1} | x_0), then computes q(x_t | x_{t+1}, x_0).\n",
    "        Args:\n",
    "          x_0: an array containing x_0 samples. These are expected to be integral\n",
    "            unless make_one_hot is False (in which case probabilities can be\n",
    "            provided).\n",
    "          t: the timestep to compute (as an int or integer array with shape that\n",
    "            matches x_0.\n",
    "          samples: if not None, use these samples to compute the posterior.\n",
    "          transition_probs: precomputed transition probabilities.\n",
    "          return_logits: if True, returns the (noisy) log of the probabilities.\n",
    "          return_transition_probs: if true, returns the transition probs as well.\n",
    "          transition_probs_in_logits: include transition probs in logits.\n",
    "          make_one_hot: if True, will convert the input to a one_hot vector.\n",
    "          epsilon: a small amount of noise to add to logits if needed.\n",
    "          step_size: if provided, computes q(x_{t + step_size} | x_0), etc. This is\n",
    "            used to sample fewer steps for ELBO evaluation on a longer trained\n",
    "            model.\n",
    "        Returns:\n",
    "          a list of samples with the same shape as x_0 and the associated posterior\n",
    "          probabilities (or logits).\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class DiscreteDiffusionMatrixBase(DiscreteDiffusionBase):\n",
    "    \"\"\"Base class for all matrix-noise schedulers.\"\"\"\n",
    "    num_steps: int\n",
    "    dim: int\n",
    "    tokenizer: Any\n",
    "\n",
    "    def get(self, t):\n",
    "        \"\"\"Returns the transition matrix q(x_{t+1} | x_t).\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def custom_product_fn(self, t):\n",
    "        \"\"\"Returns q(x_t | x_0), the product of the first t matrices.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def qt_reverse(self,\n",
    "                   qt_plus_1,\n",
    "                   t,\n",
    "                   return_logits=False,\n",
    "                   make_one_hot=False,\n",
    "                   epsilon=1e-20):\n",
    "        \"\"\"Get q(x_{t+1} | x_t), the one-step posterior efficiently.\n",
    "        Args:\n",
    "          qt_plus_1: an array of floats specifying a distribution over p(x_0).\n",
    "          t: t in q(x_{t+1} | x_t).\n",
    "          return_logits: if True, return the output logits\n",
    "          epsilon: a small number to normalize logits conversion with, if needed.\n",
    "        Returns:\n",
    "          q(x_{t+1} | x_t).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_qt_matrix(self, t):\n",
    "        \"\"\"Returns the matrix Q = q(x_t | x_0) materialized over all x_0.\"\"\"\n",
    "        if self.supports_efficient_inference():\n",
    "            return self.custom_product_fn(t)\n",
    "\n",
    "        print(\"WARNING: using inefficient matrix product.\")\n",
    "\n",
    "        # otherwise, multiply by the ith matrix in a for-loop.\n",
    "        def product_fn(i, state):\n",
    "            return torch.matmul(self.get(i), state)\n",
    "\n",
    "        final_product = fori_loop(0, t, product_fn, torch.eye(self.dim))\n",
    "\n",
    "        return final_product\n",
    "\n",
    "    def get_qt_given_q0(self,\n",
    "                        q0,\n",
    "                        t,\n",
    "                        return_logits=False,\n",
    "                        make_one_hot=False,\n",
    "                        word_freq_logits=None,\n",
    "                        epsilon=1e-20):\n",
    "        \"\"\"Get q(x_t), the n-step posterior.\n",
    "        For example, for t = 0, it returns q0 unchanged.\n",
    "        Args:\n",
    "          q0: an array of floats specifying a distribution over p(x_0).\n",
    "          t: t in q(x_t | x_0).\n",
    "          return_logits: if True, return the output logits\n",
    "          make_one_hot: if True, will convert q0 to floats if needed.\n",
    "          epsilon: a small number to normalize logits conversion with, if needed.\n",
    "        Returns:\n",
    "          q(x_t).\n",
    "        \"\"\"\n",
    "        if make_one_hot:\n",
    "            q0 = torch.nn.functional.one_hot(q0, num_classes=self.dim)\n",
    "        if self.supports_efficient_inference():\n",
    "            prob_at_time_t = torch.einsum(\"ij,...j\", self.get_qt_matrix(t), q0)\n",
    "            if return_logits:\n",
    "                return torch.log(prob_at_time_t + epsilon)\n",
    "            else:\n",
    "                return prob_at_time_t\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def sample_and_compute_posterior_q(self,\n",
    "                                       x_0,\n",
    "                                       t,\n",
    "                                       samples=None,\n",
    "                                       transition_probs=None,\n",
    "                                       return_logits=True,\n",
    "                                       return_transition_probs=False,\n",
    "                                       transition_probs_in_logits=True,\n",
    "                                       make_one_hot=True,\n",
    "                                       epsilon=1e-20,\n",
    "                                       step_size=1,\n",
    "                                       word_freq_logits=None,\n",
    "                                       ):\n",
    "        \"\"\"Samples from q(x_{t+1} | x_0), then computes q(x_t | x_{t+1}, x_0).\n",
    "        Args:\n",
    "          x_0: an array containing x_0 samples. These are expected to be integral\n",
    "            unless make_one_hot is False (in which case probabilities can be\n",
    "            provided).\n",
    "          t: the timestep to compute (as an int or integer array with shape that\n",
    "            matches x_0.\n",
    "          samples: if not None, use these samples to compute the posterior.\n",
    "          transition_probs: precomputed transition probabilities.\n",
    "          return_logits: if True, returns the (noisy) log of the probabilities.\n",
    "          return_transition_probs: if true, returns the transition probs as well.\n",
    "          transition_probs_in_logits: include transition probs in logits.\n",
    "          make_one_hot: if True, will convert the input to a one_hot vector.\n",
    "          epsilon: a small amount of noise to add to logits if needed.\n",
    "          step_size: if provided, computes q(x_{t + step_size} | x_0), etc. This is\n",
    "            used to sample fewer steps for ELBO evaluation on a longer trained\n",
    "            model.\n",
    "        Returns:\n",
    "          a list of samples with the same shape as x_0 and the associated posterior\n",
    "          probabilities (or logits).\n",
    "        \"\"\"\n",
    "        dim = self.dim\n",
    "        if make_one_hot:\n",
    "            x_0 = torch.nn.functional.one_hot(x_0, dim).reshape(x_0.shape + (dim,))\n",
    "\n",
    "        prob_at_time_t = self.get_qt_given_q0(q0=x_0, t=t, word_freq_logits=word_freq_logits)\n",
    "\n",
    "        if self.supports_efficient_get():\n",
    "            if step_size > 1:\n",
    "                transition_matrix = torch.eye(self.dim)\n",
    "\n",
    "                for i in range(step_size):\n",
    "                    transition_matrix = self.get(t + i) @ transition_matrix\n",
    "\n",
    "            else:\n",
    "                transition_matrix = self.get(t)\n",
    "\n",
    "            prob_at_time_t_plus_one = torch.einsum(\n",
    "                \"ij,...j->...i\",\n",
    "                transition_matrix,\n",
    "                prob_at_time_t,\n",
    "            )\n",
    "        else:\n",
    "            prob_at_time_t_plus_one = self.get_qt_given_q0(q0=x_0, t=t + step_size, word_freq_logits=word_freq_logits)\n",
    "\n",
    "        if samples is None and transition_probs is not None:\n",
    "            raise ValueError(\"samples were not provided but transition_probs were.\")\n",
    "\n",
    "        if samples is None:\n",
    "            logits = torch.log(prob_at_time_t_plus_one + epsilon)\n",
    "            samples = self.sample_cls.sample(logits, x_0)\n",
    "\n",
    "        if transition_probs is None:\n",
    "            if self.supports_efficient_get():\n",
    "                transition_probs = transition_matrix[samples]\n",
    "            else:\n",
    "                if step_size > 1:\n",
    "                    transition_probs = torch.nn.functional.one_hot(samples, self.dim)\n",
    "                    for i in range(step_size):\n",
    "                        transition_probs = self.qt_reverse(\n",
    "                            qt_plus_1=transition_probs,\n",
    "                            make_one_hot=False,\n",
    "                            t=t + step_size - 1 - i)\n",
    "                else:\n",
    "                    transition_probs = self.qt_reverse(qt_plus_1=samples, make_one_hot=True, t=t)\n",
    "\n",
    "        if not transition_probs_in_logits and not return_logits:\n",
    "            raise ValueError(\n",
    "                \"Cannot exclude transition probs from logits if return_logits is false.\"\n",
    "            )\n",
    "\n",
    "        if return_logits:\n",
    "            # for numerical stability, we can compute log(a*b) = log(a) + log(b)\n",
    "            posterior_logits = torch.log(prob_at_time_t + epsilon)\n",
    "\n",
    "            if transition_probs_in_logits:\n",
    "                posterior_logits = posterior_logits + torch.log(transition_probs + epsilon)\n",
    "\n",
    "            if return_transition_probs:\n",
    "                return posterior_logits, samples, transition_probs\n",
    "            else:\n",
    "                return posterior_logits, samples\n",
    "        else:\n",
    "            # here we hope this never actually sums to zero. There's a chance\n",
    "            # this will produce NaN gradients, but that's OK because they'll be\n",
    "            # skipped.\n",
    "\n",
    "            posterior = transition_probs * prob_at_time_t\n",
    "            denominator = posterior.sum(dim=-1, keepdims=True)\n",
    "            posterior = posterior / denominator\n",
    "            if return_transition_probs:\n",
    "                return posterior, samples, transition_probs\n",
    "            else:\n",
    "                return posterior, samples\n",
    "\n",
    "\n",
    "class MaskDiffusion(DiscreteDiffusionMatrixBase):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 schedule,\n",
    "                 tokenizer,\n",
    "                 use_fast_inference=True,\n",
    "                 sample_cls=None,\n",
    "                 word_freq=None,\n",
    "                 word_freq_lambda=0.,\n",
    "                 device=None,\n",
    "                 ):\n",
    "        \"\"\"A simple scheduler for masking policies.\n",
    "        Args:\n",
    "          dim: int, the dimensionality of the state space.\n",
    "          schedule: a DiffusionSchedule object for scheduling rates.\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_steps = schedule.num_steps\n",
    "        self.sample_cls = sample_cls\n",
    "        self.schedule = schedule\n",
    "        self.use_fast_inference = use_fast_inference\n",
    "        self.dim = dim  # allow mask\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.mask = torch.nn.functional.one_hot(\n",
    "                torch.tensor(self.tokenizer.mask_token_id, device=device),\n",
    "                num_classes=self.dim\n",
    "            ).unsqueeze(1).repeat(1, self.dim).float()\n",
    "        self.state = self._create_state()\n",
    "        self.word_freq = word_freq\n",
    "        import math\n",
    "        self.word_freq_lambda = word_freq_lambda * torch.sin(torch.arange(schedule.num_steps + 1, device=self.device) / schedule.num_steps * math.pi)\n",
    "\n",
    "    def _create_state(self):\n",
    "        \"\"\"Initializes values used by the get function.\"\"\"\n",
    "        betas = torch.cat((torch.tensor([0.0], device=self.device), self.schedule(torch.arange(self.num_steps, device=self.device)))).double()\n",
    "        alphas = 1 - betas\n",
    "        state = torch.cumprod(alphas, dim=0)\n",
    "        state[-1] = 0.0\n",
    "\n",
    "        return state.to(torch.float32)\n",
    "\n",
    "    def noise_fn(self, q0, t, word_freq_logits):\n",
    "        p = self.state[t]\n",
    "        if word_freq_logits is None:\n",
    "            word_freq_logits = self.word_freq.repeat(q0.size(0), 1).gather(1, q0.argmax(-1))\n",
    "            word_freq_logits = word_freq_logits - word_freq_logits.mean(-1, keepdims=True)\n",
    "\n",
    "        word_freq_probs = word_freq_logits.unsqueeze(-1) * self.word_freq_lambda[t]\n",
    "\n",
    "        p = torch.clip(p + word_freq_probs, 0., .999)\n",
    "\n",
    "        non_mask_prob = p * q0\n",
    "\n",
    "        mask_prob = 1 - non_mask_prob.sum(-1, keepdims=True) + non_mask_prob[\n",
    "            ..., self.tokenizer.mask_token_id].unsqueeze(-1)\n",
    "\n",
    "        prob_at_time_t = torch.cat((\n",
    "            non_mask_prob[..., :self.tokenizer.mask_token_id], mask_prob,\n",
    "            non_mask_prob[..., self.tokenizer.mask_token_id + 1:]\n",
    "        ), dim=-1)\n",
    "        return prob_at_time_t\n",
    "\n",
    "\n",
    "    def supports_efficient_inference(self):\n",
    "        return self.use_fast_inference\n",
    "\n",
    "    def stationary_probs(self, size):\n",
    "        stationary = torch.zeros(size=size + (self.dim,), device=self.device)\n",
    "        stationary[..., self.tokenizer.mask_token_id] = 1\n",
    "        return stationary\n",
    "\n",
    "    def sample_stationary(self, size):\n",
    "        return torch.full(size=size, fill_value=self.tokenizer.mask_token_id, device=self.device)\n",
    "\n",
    "    def custom_product_fn(self, t):\n",
    "        \"\"\"Returns product of first n matrices. Only supported for beta constant.\"\"\"\n",
    "        dim = self.dim\n",
    "\n",
    "        if self.schedule.is_constant:\n",
    "            beta = self.schedule(0)\n",
    "            one_minus_beta_t_sq = (1 - beta) ** t\n",
    "            return one_minus_beta_t_sq * torch.eye(dim) + (1 - one_minus_beta_t_sq) * self._get_mask()\n",
    "\n",
    "        else:\n",
    "            p = self.state[t]\n",
    "            return p * torch.eye(dim) + (1 - p) * self._get_mask()\n",
    "\n",
    "    def _get_mask(self):\n",
    "        return self.mask\n",
    "\n",
    "    def get(self, t):\n",
    "        beta = self.schedule(t)\n",
    "\n",
    "        return (1 - beta) * torch.eye(self.dim) + beta * self._get_mask()\n",
    "\n",
    "    def qt_reverse(self,\n",
    "                   qt_plus_1,\n",
    "                   t,\n",
    "                   return_logits=False,\n",
    "                   make_one_hot=False,\n",
    "                   epsilon=1e-20\n",
    "                   ):\n",
    "        \"\"\"Get q(x_{t+1} | x_t), the one-step posterior efficiently.\n",
    "        Args:\n",
    "          qt_plus_1: an array of floats specifying a distribution over p(x_0).\n",
    "          t: t in q(x_{t+1} | x_t).\n",
    "          return_logits: if True, return the output logits\n",
    "          make_one_hot: if True, will convert q0 to floats if needed.\n",
    "          epsilon: a small number to normalize logits conversion with, if needed.\n",
    "        Returns:\n",
    "          q(x_{t+1} | x_t).\n",
    "        \"\"\"\n",
    "        if make_one_hot:\n",
    "            assert qt_plus_1.dtype == torch.int64\n",
    "            qt_plus_1 = torch.nn.functional.one_hot(qt_plus_1, num_classes=self.dim)\n",
    "\n",
    "        beta = self.schedule(t)\n",
    "        qtpls1_at_mask = qt_plus_1[Ellipsis, self.tokenizer.mask_token_id: self.tokenizer.mask_token_id + 1]\n",
    "        non_mask_prob0 = (1 - beta) * qt_plus_1[Ellipsis, :self.tokenizer.mask_token_id] + beta * qtpls1_at_mask\n",
    "        non_mask_prob1 = (1 - beta) * qt_plus_1[Ellipsis, self.tokenizer.mask_token_id + 1:] + beta * qtpls1_at_mask\n",
    "        prob_at_time_t = torch.cat((non_mask_prob0, qtpls1_at_mask, non_mask_prob1), dim=-1)\n",
    "\n",
    "        if return_logits:\n",
    "            return torch.log(prob_at_time_t + epsilon)\n",
    "        else:\n",
    "            return prob_at_time_t\n",
    "\n",
    "    def get_qt_given_q0(self,\n",
    "                        q0,\n",
    "                        t,\n",
    "                        return_logits=False,\n",
    "                        make_one_hot=False,\n",
    "                        epsilon=1e-20,\n",
    "                        word_freq_logits=None):\n",
    "        \"\"\"Get q(x_t), the n-step posterior.\n",
    "        Can do efficiently for masks.\n",
    "        For example, for t = 0, it returns q0 unchanged.\n",
    "        Args:\n",
    "          q0: an array of floats specifying a distribution over p(x_0).\n",
    "          t: t in q(x_t | x_0).\n",
    "          return_logits: if True, return the output logits\n",
    "          epsilon: a small number to normalize logits conversion with, if needed.\n",
    "        Returns:\n",
    "          q(x_t | x_0).\n",
    "        \"\"\"\n",
    "        if not self.supports_efficient_inference():\n",
    "            return super().get_qt_given_q0(\n",
    "                q0,\n",
    "                t,\n",
    "                return_logits=return_logits,\n",
    "                epsilon=epsilon)\n",
    "\n",
    "        if make_one_hot:\n",
    "            assert q0.dtype == torch.int64\n",
    "            q0 = torch.nn.functional.one_hot(q0, num_classes=self.dim)\n",
    "\n",
    "        prob_at_time_t = q0 if t == 0 else self.noise_fn(q0, t, word_freq_logits)\n",
    "\n",
    "        if return_logits:\n",
    "            return torch.log(prob_at_time_t + epsilon)\n",
    "        else:\n",
    "            return prob_at_time_t\n",
    "\n",
    "    def supports_efficient_get(self):\n",
    "        return not self.use_fast_inference\n",
    "\n",
    "\n",
    "\n",
    "def create_discrete_diffusion_schedule(\n",
    "        kind=\"linear\",\n",
    "        beta_min=1e-3,\n",
    "        beta_max=1e-1,\n",
    "        num_steps=100,\n",
    "        scale=1.0,\n",
    "        s=0.008\n",
    "):\n",
    "    \"\"\"Creates a callable schedule object to use for diffusion rates.\n",
    "    Args:\n",
    "    kind: str, one of 'standard', 'linear', 'cosine', 'mutual_information'. If\n",
    "      standard, performs standard binomial diffusion taken from Sohl-Dicksteein\n",
    "      et al, ignoring betas. Otherwise, linear schedule between beta_min and\n",
    "      beta_max.\n",
    "    beta_min: the minimum beta. Ignored if kind == standard.\n",
    "    beta_max: the maximum beta.\n",
    "    num_steps: int, the number of steps to take.\n",
    "    scale: for standard schedule, rescales num_steps by this amount.\n",
    "    Returns:\n",
    "    a DiffusionSchedule object.\n",
    "    \"\"\"\n",
    "\n",
    "    assert beta_min <= beta_max\n",
    "    assert num_steps > 0\n",
    "    assert scale >= 1\n",
    "\n",
    "    print(f\"using standard schedule with num_steps: {num_steps}.\")\n",
    "\n",
    "    def schedule_fn(step):\n",
    "        return 1 / (num_steps - step)\n",
    "\n",
    "    return DiffusionSchedule(schedule_fn, num_steps, is_constant=False)\n",
    "\n",
    "\n",
    "def p_forward(\n",
    "        denoise_fn,\n",
    "        target_mask,\n",
    "        x_t,\n",
    "        t,\n",
    "        diffusion,\n",
    "        predict_x0=True,\n",
    "        return_x0=False,\n",
    "        return_logits=False,\n",
    "        special_case_x0=False,\n",
    "        transition_probs=None,\n",
    "        transition_probs_in_logits=True,\n",
    "        maximum_likelihood=False,\n",
    "        epsilon=1e-20,\n",
    "        step_size=1,\n",
    "        word_freq_logits=None\n",
    "):\n",
    "    \"\"\"Returns probabilities from the reverse process p(x_{t-1} | x_t).\n",
    "    Args:\n",
    "    denoise_fn: the reverse process. Must support embed, call, and attend.\n",
    "    x_t: the current value of x_t to condition on.\n",
    "    t: the timestep t.\n",
    "    diffusion: the Diffusion object to use for noise.\n",
    "    predict_x0: if True, assumes the model output corresponds to its prediction\n",
    "      for p(x_0 | x_t). Otherwise assumes model predicts p(x_{t-1} | x_t).\n",
    "    return_x0: if True, will return probs for x_0 as well as x_{t-1}.\n",
    "    return_logits: if True, will return logits instead of probabilities.\n",
    "    special_case_x0: if True, will directly predict x0 instead of using the\n",
    "      forward process probabilities.\n",
    "    transition_probs: if provided, q(x_{t+1} | x_t) probs to reuse.\n",
    "    transition_probs_in_logits: if False, will ignore transition probs in logits\n",
    "      (only allowed if return_logits is True). This is because this term is\n",
    "      independent of theta.\n",
    "    maximum_likelihood: if true, will draw the most likely x0 before applying\n",
    "      the forward process.\n",
    "    epsilon: a small number.\n",
    "    step_size: step size to compute posterior from.\n",
    "    Returns:\n",
    "    probabilities for q(x_{t-1} | x_t) (and probabilities for x0 if predict_x0\n",
    "    is True)\n",
    "    \"\"\"\n",
    "    assert not (step_size > 1 and not predict_x0)\n",
    "\n",
    "    logits = denoise_fn(targets=x_t, timestep=t, attention_mask=target_mask)\n",
    "    probs = torch.nn.Softmax(dim=-1)(logits)\n",
    "\n",
    "    if not predict_x0:\n",
    "        retval = logits if return_logits else probs\n",
    "        if return_x0:\n",
    "            return retval, None\n",
    "        else:\n",
    "            return retval\n",
    "\n",
    "    if maximum_likelihood:\n",
    "        probs = probs.argmax(-1)\n",
    "\n",
    "    # we use this to compute p(x_{t-1} | x_t) = sum_x0 q(x_{t-1} | x_t, x_0)\n",
    "    # p(x_0 | x_t).\n",
    "    qt_probs, _ = diffusion.sample_and_compute_posterior_q(\n",
    "        x_0=probs,\n",
    "        t=t - step_size,\n",
    "        return_logits=return_logits,\n",
    "        make_one_hot=maximum_likelihood,\n",
    "        transition_probs_in_logits=transition_probs_in_logits,\n",
    "        transition_probs=transition_probs,\n",
    "        samples=x_t,\n",
    "        epsilon=epsilon,\n",
    "        step_size=step_size,\n",
    "        word_freq_logits=word_freq_logits\n",
    "    )\n",
    "\n",
    "    retval_x0 = logits if return_logits else probs\n",
    "    retval = qt_probs\n",
    "\n",
    "    # we can special case t = 1 to just use the raw logits outputs.\n",
    "    mask = ((t == step_size) & special_case_x0).long()\n",
    "    retval = mask * retval_x0 + (1 - mask) * retval\n",
    "    # retval = retval_x0 if t == step_size else retval\n",
    "\n",
    "    if return_x0:\n",
    "        return retval, retval_x0\n",
    "    else:\n",
    "        return retval\n",
    "\n",
    "\n",
    "def compute_prior_kl(x_start, diffusion, target_mask=None, word_freq_logits=None):\n",
    "    \"\"\"Computes KL divergence between q(x_T) and the true distribution.\"\"\"\n",
    "\n",
    "    num_steps = diffusion.num_steps\n",
    "\n",
    "    q_probs = diffusion.get_qt_given_q0(q0=x_start, t=num_steps, return_logits=False, make_one_hot=True, word_freq_logits=word_freq_logits)  # get end step\n",
    "    p_probs = diffusion.stationary_probs(q_probs.shape[:-1])\n",
    "\n",
    "    loss = losses.kl_divergence_with_probs(q_probs, p_probs)\n",
    "\n",
    "    if target_mask is not None:\n",
    "        loss = (loss * target_mask).sum()\n",
    "    else:\n",
    "        loss = loss.sum()\n",
    "\n",
    "    return loss, 1\n",
    "\n",
    "\n",
    "def compute_kl_reverse_process(x_start,\n",
    "                               t,\n",
    "                               *,\n",
    "                               diffusion,\n",
    "                               denoise_fn,\n",
    "                               predict_x0=True,\n",
    "                               log_space=False,\n",
    "                               hybrid_lambda=0.0,\n",
    "                               use_cached_transition=True,\n",
    "                               target_mask=None,\n",
    "                               word_freq_logits=None,\n",
    "                               step_size=1,\n",
    "                               device=None):\n",
    "    \"\"\"Returns the KL for one term in the ELBO (time t) (loss L_t).\n",
    "    This assumes x_start is a sample from x_0, from which we draw samples from\n",
    "    q(x_t | x_0) and then compute q(x_{t-1} | x_t, x_0) following the LaTeX. This\n",
    "    is the KL divergence for terms L_1 through L_{T-1}.\n",
    "    Args:\n",
    "    x_start: a sample from p(data) (or q(x_0)).\n",
    "    t: the loss term to compute.\n",
    "    diffusion: the diffusion object to use.\n",
    "    denoise_fn: a functool.partial-ed version of the model_apply function which\n",
    "      takes a set of targets (x_t) and noise level and returns q(x_{t-1} | x_t,\n",
    "      x_0).\n",
    "    predict_x0: if True, will predict a distribution over x0 instead of x_{t-1}.\n",
    "    log_space: if True, will perform the loss calculations in log space.\n",
    "    label_smoothing: label smoothing for cross entropy.\n",
    "    hybrid_lambda: coefficient for hybrid cross-entropy loss.\n",
    "    use_cached_transition: if True, will reuse q(x_{t+1} | x_t) computation.\n",
    "    target_mask: mask for target sequence.\n",
    "    step_size: the step size over which the ELBO is computed.\n",
    "    Returns:\n",
    "    the KL divergence and denominator.\n",
    "    \"\"\"\n",
    "\n",
    "    if step_size > 1 and not predict_x0:\n",
    "        raise ValueError(\"cannot skip steps when not predicting x0.\")\n",
    "\n",
    "    # sample from q(x_{t+1} | x_start), then compute q(x_t | x_{t+1}, x_start)\n",
    "    # q_t and p_t can be logits or probs depending on log_space.\n",
    "    q_t, x_t_plus_1, transition_probs = diffusion.sample_and_compute_posterior_q(\n",
    "        x_start,\n",
    "        t,\n",
    "        return_logits=log_space,\n",
    "        return_transition_probs=True,\n",
    "        step_size=step_size,\n",
    "        word_freq_logits=word_freq_logits,\n",
    "    )\n",
    "\n",
    "    transition_probs = transition_probs if use_cached_transition else None\n",
    "\n",
    "    p_t = p_forward(\n",
    "        denoise_fn,\n",
    "        target_mask,\n",
    "        x_t_plus_1,\n",
    "        t + step_size,\n",
    "        diffusion,\n",
    "        predict_x0=predict_x0,\n",
    "        return_x0=predict_x0 and hybrid_lambda > 0.0,\n",
    "        return_logits=log_space,\n",
    "        transition_probs=transition_probs,\n",
    "        step_size=step_size,\n",
    "        word_freq_logits=word_freq_logits\n",
    "    )\n",
    "\n",
    "    if predict_x0 and hybrid_lambda > 0.0:\n",
    "        p_t, p_0 = p_t\n",
    "        if log_space:\n",
    "            cross_entropy = losses.cross_entropy_with_logits(logits=p_0, targets=x_start)\n",
    "        else:\n",
    "            cross_entropy = losses.cross_entropy_with_probs(probs=p_0, targets=x_start)\n",
    "\n",
    "        hybrid_loss = hybrid_lambda * cross_entropy\n",
    "    else:\n",
    "        hybrid_loss = torch.tensor([0.], device=device)\n",
    "\n",
    "    if log_space:\n",
    "        kl = losses.kl_divergence_with_logits(q_t, p_t)\n",
    "        cross_entropy = losses.cross_entropy_with_logits(logits=p_t, targets=x_start)\n",
    "    else:\n",
    "        kl = losses.kl_divergence_with_probs(q_t, p_t)\n",
    "        cross_entropy = losses.cross_entropy_with_probs(probs=p_t, targets=x_start)\n",
    "\n",
    "    if target_mask is not None:\n",
    "        kl = (kl * target_mask).sum()\n",
    "        cross_entropy = (cross_entropy * target_mask).sum()\n",
    "        hybrid_loss = (hybrid_loss * target_mask).sum()\n",
    "    else:\n",
    "        kl = kl.sum()\n",
    "        cross_entropy = cross_entropy.sum()\n",
    "        hybrid_loss = hybrid_loss.sum()\n",
    "\n",
    "    mask = (t == 0).long()\n",
    "    base_loss = mask * cross_entropy + (1 - mask) * kl\n",
    "    loss = base_loss + hybrid_loss\n",
    "    denominator = 1\n",
    "    metrics_dict = {\n",
    "        \"loss\": loss,\n",
    "        \"denominator\": denominator,\n",
    "        \"hybrid_loss\": hybrid_loss,\n",
    "        \"base_loss\": base_loss,\n",
    "        \"cross_entropy_loss\": cross_entropy,\n",
    "        \"t0_loss\": mask * cross_entropy,\n",
    "        \"kl_loss\": kl,\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "\n",
    "def discrete_diffusion_elbo(\n",
    "        x_start,\n",
    "        *,\n",
    "        denoise_fn,\n",
    "        diffusion,\n",
    "        target_mask,\n",
    "        word_freq_logits,\n",
    "        predict_x0=True,\n",
    "        length_probs=None,\n",
    "        normalize_without_padding=True,\n",
    "        eval_step_size=1,\n",
    "        device=None,\n",
    "):\n",
    "    \"\"\"Computes the ELBO likelihood bound for discrete diffusion models.\n",
    "    Pseudocode:\n",
    "    1. starting at t = T and going towards t = 0:\n",
    "    2. sample P(x_t | x_0)\n",
    "    3. use NN to compute P(x_{t-1} | x_t)\n",
    "    4. get q(x_{t-1} | x_t, x_0)\n",
    "    5. compute KL divergence\n",
    "    6. At T = 0, get discrete log likelihoods\n",
    "    Args:\n",
    "    x_start: data point.\n",
    "    denoise_fn: the denoise_fn function (including params).\n",
    "    diffusion: the noise schedule object.\n",
    "    target_mask: mask for padding targets\n",
    "    predict_x0: if True, assumes the neural net predicts x0.\n",
    "    length_probs: list of probabilities for each sequence length.\n",
    "    normalize_without_padding: if True, ignore padding when normalizing.\n",
    "    eval_step_size: step size for evaluation.\n",
    "    return_all_likelihoods: if True, will return all likelihoods for all timesteps.\n",
    "    Returns:\n",
    "    the full ELBO bound.\n",
    "    \"\"\"\n",
    "    assert diffusion.num_steps % eval_step_size == 0\n",
    "    assert diffusion.num_steps > eval_step_size\n",
    "\n",
    "    @dataclass\n",
    "    class State:\n",
    "        t: Any\n",
    "        log_likelihood: Any\n",
    "\n",
    "    def elbo_body_fn(state, _):\n",
    "        metrics_dict = compute_kl_reverse_process(\n",
    "            x_start,\n",
    "            state.t,\n",
    "            denoise_fn=denoise_fn,\n",
    "            diffusion=diffusion,\n",
    "            predict_x0=predict_x0,\n",
    "            target_mask=target_mask,\n",
    "            hybrid_lambda=0.0,\n",
    "            step_size=eval_step_size,\n",
    "            word_freq_logits=word_freq_logits,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        log_likelihood = metrics_dict[\"base_loss\"] / metrics_dict[\"denominator\"]\n",
    "\n",
    "        return State(\n",
    "            t=state.t - eval_step_size,\n",
    "            log_likelihood=state.log_likelihood + log_likelihood,\n",
    "        ), None\n",
    "\n",
    "    init_state = State(\n",
    "        t=torch.tensor([diffusion.num_steps - eval_step_size], device=device),\n",
    "        log_likelihood=torch.tensor(0.0, device=device),\n",
    "    )\n",
    "\n",
    "    num_steps = diffusion.num_steps // eval_step_size\n",
    "\n",
    "    final_state, _ = scan(elbo_body_fn, init_state, None, num_steps)\n",
    "\n",
    "    log_likelihood = final_state.log_likelihood\n",
    "\n",
    "    prior, denominator = compute_prior_kl(x_start, diffusion, target_mask=target_mask, word_freq_logits=word_freq_logits)\n",
    "\n",
    "    if target_mask is not None:\n",
    "        target_length = torch.count_nonzero(target_mask)\n",
    "    else:\n",
    "        target_length = None\n",
    "\n",
    "    if length_probs is not None:\n",
    "        length_probs = torch.tensor(length_probs, device=device)\n",
    "        length_log_likelihood = -torch.log(length_probs[target_length])\n",
    "    else:\n",
    "        length_log_likelihood = 0.0\n",
    "\n",
    "    elbo = log_likelihood + length_log_likelihood + prior / denominator\n",
    "\n",
    "    elbo_length = target_length if normalize_without_padding else x_start.size(-1)\n",
    "\n",
    "    return {\n",
    "        \"elbo\": elbo,\n",
    "        \"elbo_in_bits_per_dim\": elbo / (np.log(2) * elbo_length),\n",
    "        \"likelihood\": log_likelihood,\n",
    "        \"prior\": prior,\n",
    "        \"length_likelihood\": length_log_likelihood,\n",
    "        \"nn/num_steps\": num_steps,\n",
    "    }\n",
    "\n",
    "\n",
    "def discrete_diffusion_predict_fn(\n",
    "    shape,\n",
    "    denoise_fn,\n",
    "    diffusion,\n",
    "    target_mask=None,\n",
    "    predict_x0=False,\n",
    "    use_maximum_likelihood_decoding=False,\n",
    "    step_size=1,\n",
    "    topk=0,\n",
    "    topp=-1.0,\n",
    "    context_fn=None,\n",
    "    sample_cls=None,\n",
    "    show_process=False,\n",
    "    temperature=1.0\n",
    "):\n",
    "    \"\"\"Predict an image or text from a diffusion model.\n",
    "\n",
    "  Args:\n",
    "    params: a PyTree of parameters for the model.\n",
    "    rng_key: an RNG key.\n",
    "    targets: ignored, used for shape info.\n",
    "    model: the Flax model to use.\n",
    "    dataset_info: the Problem object for the current task.\n",
    "    diffusion: the noise schedule to use to condition the prediction steps.\n",
    "    diffusion_state: if provided, a state object used by the diffusion class.\n",
    "    inputs: if provided, used to condition the prediction.\n",
    "    return_intermediates: if True, uses lax.scan to return all intermediate\n",
    "      steps in the reverse process.\n",
    "    predict_x0: if True, will predict a distribution over x_0 instead of x_{t-1}\n",
    "      which allows for the number of inference steps to be varied after\n",
    "      training.\n",
    "    use_maximum_likelihood_decoding: if True, will take the maximum likelihood\n",
    "      sample instead of sampling from the posterior. Will tend to produce very\n",
    "      trivial results, unless predict_x0 is True.\n",
    "    mask_padding: if True, mask out padding tokens.\n",
    "    predict_completions: if True, instead of predicting from x_T, predict from\n",
    "      other points x_t for each possible t. Returns different metrics and\n",
    "      shapes.\n",
    "    step_size: tne size of each inference step (step_size > 1 skips steps).\n",
    "\n",
    "  Returns:\n",
    "    a dictionary containing metrics and information about the prediction\n",
    "      process.\n",
    "  \"\"\"\n",
    "    if show_process:\n",
    "        tk = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    num_steps = diffusion.num_steps\n",
    "    assert num_steps % step_size == 0\n",
    "    assert step_size < num_steps\n",
    "\n",
    "    @dataclass\n",
    "    class SamplingState:\n",
    "        x: torch.Tensor  # current predicted seqeunce\n",
    "        x0: Any  # only used if predict_x0 is true\n",
    "        t: int  # current step\n",
    "\n",
    "\n",
    "    length = shape[-1]\n",
    "\n",
    "\n",
    "    def sampling_step(step, state):\n",
    "        del step\n",
    "\n",
    "        t = state.t  # initially, num_steps, and decreases from there.\n",
    "\n",
    "        logits, x0_logits = p_forward(\n",
    "            denoise_fn,\n",
    "            target_mask,\n",
    "            x_t=state.x,\n",
    "            t=t,\n",
    "            diffusion=diffusion,\n",
    "            predict_x0=predict_x0,\n",
    "            return_x0=True,\n",
    "            return_logits=True,\n",
    "            maximum_likelihood=use_maximum_likelihood_decoding,\n",
    "            step_size=step_size\n",
    "        )\n",
    "\n",
    "        if x0_logits is not None:\n",
    "            x0 = x0_logits.argmax(-1)\n",
    "        else:\n",
    "            x0 = None\n",
    "\n",
    "        # logits = torch.nn.functional.gumbel_softmax(logits, tau=2)\n",
    "\n",
    "        logits = logits / temperature\n",
    "        logits = top_k_top_p_filtering(logits, top_k=topk, top_p=topp)\n",
    "\n",
    "        sample = torch.distributions.categorical.Categorical(logits=logits).sample()\n",
    "        if show_process:\n",
    "            print(tk.batch_decode(x0, clean_up_tokenization_spaces=False))\n",
    "\n",
    "        return SamplingState(x=sample, x0=x0, t=t - step_size)\n",
    "\n",
    "    x = diffusion.sample_stationary(shape)\n",
    "    if context_fn is not None:\n",
    "        x = context_fn(x)\n",
    "\n",
    "    if predict_x0:\n",
    "        init_state = SamplingState(x, x, torch.tensor([num_steps], device=self.device))\n",
    "    else:\n",
    "        init_state = SamplingState(x, None, torch.tensor([num_steps], device=self.device))\n",
    "\n",
    "    total_steps = num_steps // step_size\n",
    "\n",
    "    final_state = fori_loop(0, total_steps, sampling_step, init_state)\n",
    "\n",
    "    predictions = {\n",
    "        \"final_state\": final_state.x,\n",
    "        \"initial_state\": init_state.x,\n",
    "        \"scalar/num_steps\": num_steps,\n",
    "        \"scalar/length\": length,\n",
    "        \"scalar/total_steps\": total_steps,\n",
    "    }\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c9e3f8a-edf9-4969-bd2d-0ea751558453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using standard schedule with num_steps: 2048.\n"
     ]
    }
   ],
   "source": [
    "diffusion_schedule = create_discrete_diffusion_schedule(\"mutual\", num_steps=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3df74182-7e77-4177-a4bb-1fdb08dce309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiffusionSchedule(steps: 2048, is_constant: False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50ce72fe-0e79-4dda-b3af-f8bca131d94b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m diffusion_instance \u001b[38;5;241m=\u001b[39m MaskDiffusion(\n\u001b[0;32m----> 2\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m      3\u001b[0m     schedule\u001b[38;5;241m=\u001b[39mdiffusion_schedule,\n\u001b[1;32m      4\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m      5\u001b[0m     sample_cls\u001b[38;5;241m=\u001b[39msample_cls,\n\u001b[1;32m      6\u001b[0m     word_freq_lambda\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mword_freq_lambda,\n\u001b[1;32m      7\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "diffusion_instance = MaskDiffusion(\n",
    "    dim=tokenizer.vocab_size,\n",
    "    schedule=diffusion_schedule,\n",
    "    tokenizer=tokenizer,\n",
    "    sample_cls=sample_cls,\n",
    "    word_freq_lambda=args.word_freq_lambda,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e10dfaea-d96d-4e6d-ba4a-816f17108ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8229eedb-8beb-460f-a09f-90fce03df57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33dd18-74f9-4d61-8089-1274e4f145f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d834771b-20bb-4974-8e4b-dba69fc6f408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
