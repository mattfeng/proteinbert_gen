{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e5a8ca-9eae-41a7-a1be-cde0d463b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ecf403a-9e4d-48a3-9673-9b35a73c1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW, SGD\n",
    "from collections import namedtuple\n",
    "\n",
    "import proteinbert_gen.constants as consts\n",
    "import proteinbert_gen.mask_diffusion as mask_diffusion\n",
    "\n",
    "from proteinbert_gen.debugging import print2\n",
    "from proteinbert_gen.proteinbert import ProteinBERT, load_pretrained_weights\n",
    "from proteinbert_gen.word_freq import create_word_freq_tensor\n",
    "from proteinbert_gen.tokenizer import ProteinTokenizer\n",
    "from proteinbert_gen.dataset import sprot_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe024125-8e53-4eed-817a-baff253eed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmattfeng\u001b[0m (\u001b[33mkaiogenbio\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "Hyperparameters = namedtuple(\n",
    "    \"Hyperparameters\",\n",
    "    [\n",
    "        \"batch_size\",\n",
    "        \"epochs\",\n",
    "        \"num_steps\",\n",
    "        \"word_freq_lambda\",\n",
    "        \"device\",\n",
    "        \"hybrid_lambda\",\n",
    "        \"lr\",\n",
    "        \"logging_steps\",\n",
    "        \"eval_step_size\",\n",
    "        \"clip_grad\",\n",
    "        \"clip_grad_val\",\n",
    "        \"warmup_scheduler\",\n",
    "        \"optimizer_cls\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "args = Hyperparameters(\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    num_steps=2048,\n",
    "    word_freq_lambda=0.3,\n",
    "    device=\"cuda\",\n",
    "    hybrid_lambda=1e-4,\n",
    "    lr=1e-3,\n",
    "    logging_steps=25,\n",
    "    eval_step_size=4,\n",
    "    clip_grad_val=10,\n",
    "    clip_grad=False,\n",
    "    warmup_scheduler=False,\n",
    "    optimizer_cls=AdamW\n",
    ")\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"proteinbert_gen\",\n",
    "    config={k:str(v) for k, v in args._asdict().items()},\n",
    "    # mode=\"disabled\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e9acb-8320-4ef8-8f97-907446412932",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleClassBase(abc.ABC):\n",
    "    def sample(self, logits, x_0):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def post_process_sample_in_prediction(self, sample, x_0):\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Categorical(SampleClassBase):\n",
    "    def sample(self, logits, x_0):\n",
    "        return torch.distributions.categorical.Categorical(logits=logits).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae28cd3c-6a7d-4602-802f-bdba23dbc6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq_preprocess_fn(wf):\n",
    "    wf = wf + 1\n",
    "    wf = wf.log()\n",
    "    wf = wf / wf.max()\n",
    "\n",
    "    # range: 0 - 1\n",
    "    return wf\n",
    "\n",
    "def process_fn_in_collate(wf):\n",
    "    return wf - wf.mean()\n",
    "\n",
    "\n",
    "tokenizer = ProteinTokenizer()\n",
    "wf_tensor = create_word_freq_tensor(\"../data/sprot_1m_word_freq_dict.pkl\", tokenizer.ALL_TOKENS)\n",
    "# wf_tensor[tokenizer.mask_token_id] = 0\n",
    "wf_tensor[tokenizer.pad_token_id] = 0\n",
    "wf_tensor = word_freq_preprocess_fn(wf_tensor)\n",
    "wf_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc04c6-8aeb-416a-a825-77e930ce4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch_input, *, tokenizer, word_freq: torch.Tensor):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    word_freq_logits = []\n",
    "    \n",
    "    for item in batch_input:\n",
    "        seq = item[\"seq\"]\n",
    "        ids = torch.tensor(tokenizer.tokenize(seq))\n",
    "        mask = torch.ones_like(ids)\n",
    "        logits = process_fn_in_collate(\n",
    "            word_freq.gather(0, ids)\n",
    "        )\n",
    "        \n",
    "        input_ids.append(ids)\n",
    "        attention_mask.append(mask)\n",
    "        word_freq_logits.append(logits)\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True)\n",
    "    word_freq_logits = pad_sequence(word_freq_logits, batch_first=True)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"word_freq_logits\": word_freq_logits\n",
    "    }\n",
    "\n",
    "collate_fn = partial(collate, tokenizer=tokenizer, word_freq=wf_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec62cd-06a3-4a73-ab53-9ee7919a5e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    sprot_train,\n",
    "    batch_size=args.batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb10d7d-dfae-468b-8fa4-d015b0ef4d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(train_loader))\n",
    "print(sample_batch)\n",
    "print(sample_batch[\"input_ids\"].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8b942d-1faa-4bc3-ae1d-421414e5f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(targets, timestep, attention_mask, *, model):\n",
    "    ret = model(targets)\n",
    "    #ret = model(targets, attention_mask=attention_mask)\n",
    "    # print(\"denoise output:\", ret.shape)\n",
    "    return ret\n",
    "\n",
    "with open(\"../weights/epoch_92400_sample_23500000.pkl\", \"rb\") as f:\n",
    "    _, pretrained_model_weights, _ = pickle.load(f)\n",
    "\n",
    "model = ProteinBERT(tokenizer.vocab_size, consts.GO_ANN_SIZE)\n",
    "print(model)\n",
    "\n",
    "trainable_params = load_pretrained_weights(model, pretrained_model_weights)\n",
    "model = model.to(args.device)\n",
    "denoise_fn = partial(denoise, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f059ff8-7325-472b-849e-7e61f881f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = args.optimizer_cls(trainable_params, lr=args.lr)\n",
    "if args.warmup_scheduler:\n",
    "    warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda n: n / 10000. + 1e-3 if n < 10000 else 100. / math.sqrt(n)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497e45a-ddc8-4618-8139-3a7b25304847",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cls = Categorical()\n",
    "\n",
    "diffusion_schedule = mask_diffusion.create_discrete_diffusion_schedule(num_steps=args.num_steps)\n",
    "diffusion_instance = mask_diffusion.MaskDiffusion(\n",
    "    dim=tokenizer.vocab_size,\n",
    "    schedule=diffusion_schedule,\n",
    "    tokenizer=tokenizer,\n",
    "    sample_cls=sample_cls,\n",
    "    word_freq_lambda=args.word_freq_lambda,\n",
    "    device=args.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410a448-cb86-44f9-be96-17fd4847930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = 0.\n",
    "has_nan_log = 0\n",
    "nan_count = 0\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# def _save_output(module, grad_input, grad_output):\n",
    "#     print(module, grad_output)\n",
    "#     print(torch.isnan(grad_output[0]).any())\n",
    "#     print()\n",
    "\n",
    "# for name, module in model.named_modules():\n",
    "#     if str(type(module)).find(\"LayerNorm\") != -1:\n",
    "#         print(name)\n",
    "#         module.register_full_backward_hook(_save_output)\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        diffusion_t = diffusion_instance.sample_t()\n",
    "        # print(diffusion_t)\n",
    "\n",
    "        metrics = mask_diffusion.compute_kl_reverse_process(\n",
    "            batch[\"input_ids\"].to(args.device),\n",
    "            diffusion_t,\n",
    "            denoise_fn=denoise_fn,\n",
    "            diffusion=diffusion_instance,\n",
    "            target_mask=batch[\"attention_mask\"].to(args.device),\n",
    "            hybrid_lambda=args.hybrid_lambda,\n",
    "            predict_x0=False,\n",
    "            word_freq_logits=batch[\"word_freq_logits\"].to(args.device),\n",
    "            device=args.device\n",
    "        )\n",
    "\n",
    "        # print(metrics)\n",
    "\n",
    "        loss = metrics[\"loss\"] / args.batch_size / batch[\"input_ids\"].size(1)\n",
    "\n",
    "        if loss.isnan():\n",
    "            nan_count += 1\n",
    "            if i % args.logging_steps == args.logging_steps - 1:\n",
    "                run.log({\"nan_count\": nan_count})\n",
    "            continue\n",
    "            \n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        if args.clip_grad:\n",
    "            torch.nn.utils.clip_grad_value_(trainable_params, args.clip_grad_val)\n",
    "        \n",
    "        has_nan = 0\n",
    "        for param in trainable_params:\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    param.grad = torch.nan_to_num(param.grad, nan=0.0)\n",
    "                    has_nan = 1\n",
    "\n",
    "        has_nan_log += has_nan\n",
    "        \n",
    "        optimizer.step()\n",
    "        if args.warmup_scheduler:\n",
    "            warmup_scheduler.step()\n",
    "\n",
    "        if i % args.logging_steps == args.logging_steps - 1:\n",
    "            run.log(metrics, commit=False)\n",
    "            if args.warmup_scheduler:\n",
    "                run.log({\"last_lr\": warmup_scheduler.get_last_lr()}, commit=False)\n",
    "            run.log({\"nan_count\": nan_count, \"nan -> zero\": has_nan_log})\n",
    "            has_nan_log = 0\n",
    "\n",
    "    # generate some proteins\n",
    "    generated = mask_diffusion.discrete_diffusion_predict_fn((8, 100), denoise_fn, diffusion_instance, topk=6, topp=1.0)\n",
    "    for genseq in generated[\"final_state\"].tolist():\n",
    "        print(tokenizer.untokenize(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c63062-03b8-404c-858b-a0c7c2753c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = mask_diffusion.discrete_diffusion_predict_fn((1, 100), denoise_fn, diffusion_instance, topp=1.0)\n",
    "tokenizer.untokenize(generated[\"final_state\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f31f6-aa7e-4d97-bfc5-4aca39c292fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"../checkpoints/{run.name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c50bad-3ef8-4a5d-89a8-00bfd26d00dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
