{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e5a8ca-9eae-41a7-a1be-cde0d463b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ecf403a-9e4d-48a3-9673-9b35a73c1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW, SGD\n",
    "from collections import namedtuple\n",
    "\n",
    "import proteinbert_gen.constants as consts\n",
    "import proteinbert_gen.mask_diffusion as mask_diffusion\n",
    "\n",
    "from proteinbert_gen.debugging import print2\n",
    "from proteinbert_gen.proteinbert import ProteinBERT, load_pretrained_weights\n",
    "from proteinbert_gen.word_freq import create_word_freq_tensor\n",
    "from proteinbert_gen.tokenizer import ProteinTokenizer\n",
    "from proteinbert_gen.dataset import sprot_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe024125-8e53-4eed-817a-baff253eed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmattfeng\u001b[0m (\u001b[33mkaiogenbio\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matt/proteinbert_gen/notebooks/wandb/run-20240513_132210-neblxkz1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaiogenbio/proteinbert_gen/runs/neblxkz1' target=\"_blank\">peachy-grass-8</a></strong> to <a href='https://wandb.ai/kaiogenbio/proteinbert_gen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaiogenbio/proteinbert_gen' target=\"_blank\">https://wandb.ai/kaiogenbio/proteinbert_gen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaiogenbio/proteinbert_gen/runs/neblxkz1' target=\"_blank\">https://wandb.ai/kaiogenbio/proteinbert_gen/runs/neblxkz1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kaiogenbio/proteinbert_gen/runs/neblxkz1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ee2472d0d10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hyperparameters = namedtuple(\n",
    "    \"Hyperparameters\",\n",
    "    [\n",
    "        \"batch_size\",\n",
    "        \"epochs\",\n",
    "        \"num_steps\",\n",
    "        \"word_freq_lambda\",\n",
    "        \"device\",\n",
    "        \"hybrid_lambda\",\n",
    "        \"lr\",\n",
    "        \"logging_steps\",\n",
    "        \"eval_step_size\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "args = Hyperparameters(\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    num_steps=2048,\n",
    "    word_freq_lambda=0.3,\n",
    "    device=\"cuda\",\n",
    "    hybrid_lambda=1e-2,\n",
    "    lr=5e-4,\n",
    "    logging_steps=10,\n",
    "    eval_step_size=4\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    project=\"proteinbert_gen\",\n",
    "    config=args._asdict(),\n",
    "    # mode=\"disabled\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "178e9acb-8320-4ef8-8f97-907446412932",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleClassBase(abc.ABC):\n",
    "    def sample(self, logits, x_0):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def post_process_sample_in_prediction(self, sample, x_0):\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Categorical(SampleClassBase):\n",
    "    def sample(self, logits, x_0):\n",
    "        return torch.distributions.categorical.Categorical(logits=logits).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae28cd3c-6a7d-4602-802f-bdba23dbc6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9930, 0.8760, 0.9626, 0.9751, 0.9424, 0.9824, 0.9082, 0.9710, 0.9673,\n",
       "        1.0000, 0.9151, 0.9418, 0.9518, 0.9393, 0.9652, 0.9708, 0.9611, 0.3411,\n",
       "        0.9802, 0.8615, 0.0000, 0.9240, 0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_freq_preprocess_fn(wf):\n",
    "    wf = wf + 1\n",
    "    wf = wf.log()\n",
    "    wf = wf / wf.max()\n",
    "\n",
    "    # range: 0 - 1\n",
    "    return wf\n",
    "\n",
    "def process_fn_in_collate(wf):\n",
    "    return wf - wf.mean()\n",
    "\n",
    "\n",
    "tokenizer = ProteinTokenizer()\n",
    "wf_tensor = create_word_freq_tensor(\"../data/sprot_1m_word_freq_dict.pkl\", tokenizer.ALL_TOKENS)\n",
    "wf_tensor[tokenizer.mask_token_id] = 0\n",
    "wf_tensor = word_freq_preprocess_fn(wf_tensor)\n",
    "wf_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ddc04c6-8aeb-416a-a825-77e930ce4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch_input, *, tokenizer, word_freq: torch.Tensor):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    word_freq_logits = []\n",
    "    \n",
    "    for item in batch_input:\n",
    "        seq = item[\"seq\"]\n",
    "        ids = torch.tensor(tokenizer.tokenize(seq))\n",
    "        mask = torch.ones_like(ids)\n",
    "        logits = process_fn_in_collate(\n",
    "            word_freq.gather(0, ids)\n",
    "        )\n",
    "        \n",
    "        input_ids.append(ids)\n",
    "        attention_mask.append(mask)\n",
    "        word_freq_logits.append(logits)\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True)\n",
    "    word_freq_logits = pad_sequence(word_freq_logits, batch_first=True)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"word_freq_logits\": word_freq_logits\n",
    "    }\n",
    "\n",
    "collate_fn = partial(collate, tokenizer=tokenizer, word_freq=wf_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adec62cd-06a3-4a73-ab53-9ee7919a5e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    sprot_train,\n",
    "    batch_size=args.batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbb10d7d-dfae-468b-8fa4-d015b0ef4d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[23, 10,  0,  ..., 25, 25, 25],\n",
       "         [23, 10, 16,  ..., 25, 25, 25],\n",
       "         [23, 10,  8,  ..., 25, 25, 25],\n",
       "         ...,\n",
       "         [23, 10,  0,  ..., 25, 25, 25],\n",
       "         [23, 10,  3,  ..., 25, 25, 25],\n",
       "         [23, 10, 15,  ..., 25, 25, 25]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'word_freq_logits': tensor([[-0.9607, -0.0456,  0.0322,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.9570, -0.0418,  0.0042,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.9610, -0.0459,  0.0063,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-0.9601, -0.0449,  0.0329,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.9164, -0.0013,  0.0587,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.9635, -0.0484,  0.0073,  ...,  0.0000,  0.0000,  0.0000]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be8b942d-1faa-4bc3-ae1d-421414e5f5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProteinBERT(\n",
      "  (embed_local): Embedding(26, 128)\n",
      "  (embed_global): Sequential(\n",
      "    (0): Linear(in_features=8943, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-5): 6 x TransformerLikeBlock(\n",
      "      (wide_and_narrow_conv1d): ConvBlock(\n",
      "        (conv_narrow): Sequential(\n",
      "          (0): Rearrange('b l d -> b d l')\n",
      "          (1): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=same)\n",
      "          (2): GELU(approximate='none')\n",
      "          (3): Rearrange('b d l -> b l d')\n",
      "        )\n",
      "        (conv_wide): Sequential(\n",
      "          (0): Rearrange('b l d -> b d l')\n",
      "          (1): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=same, dilation=(5,))\n",
      "          (2): GELU(approximate='none')\n",
      "          (3): Rearrange('b d l -> b l d')\n",
      "        )\n",
      "      )\n",
      "      (dense_and_broadcast): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Rearrange('b d -> b () d')\n",
      "      )\n",
      "      (local_ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (local_dense): Sequential(\n",
      "        (0): Residual(\n",
      "          (fn): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (global_dense1): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "      )\n",
      "      (global_attention): GlobalAttention(\n",
      "        (to_q): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (1): Tanh()\n",
      "        )\n",
      "        (to_k): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=False)\n",
      "          (1): Tanh()\n",
      "        )\n",
      "        (to_v): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (1): GELU(approximate='none')\n",
      "        )\n",
      "      )\n",
      "      (global_ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (global_dense2): Sequential(\n",
      "        (0): Residual(\n",
      "          (fn): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (local_head): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=26, bias=True)\n",
      "  )\n",
      "  (global_head): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=8943, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n",
      "embed_local\n",
      "embed_global\n",
      "embed_global.0\n",
      "embed_global.1\n",
      "blocks\n",
      "blocks.0\n",
      "blocks.0.wide_and_narrow_conv1d\n",
      "blocks.0.wide_and_narrow_conv1d.conv_narrow\n",
      "blocks.0.wide_and_narrow_conv1d.conv_narrow.0\n",
      "blocks.0.wide_and_narrow_conv1d.conv_narrow.1\n",
      "blocks.0.wide_and_narrow_conv1d.conv_narrow.2\n",
      "blocks.0.wide_and_narrow_conv1d.conv_narrow.3\n",
      "blocks.0.wide_and_narrow_conv1d.conv_wide\n",
      "blocks.0.wide_and_narrow_conv1d.conv_wide.0\n",
      "blocks.0.wide_and_narrow_conv1d.conv_wide.1\n",
      "blocks.0.wide_and_narrow_conv1d.conv_wide.2\n",
      "blocks.0.wide_and_narrow_conv1d.conv_wide.3\n",
      "blocks.0.dense_and_broadcast\n",
      "blocks.0.dense_and_broadcast.0\n",
      "blocks.0.dense_and_broadcast.1\n",
      "blocks.0.dense_and_broadcast.2\n",
      "blocks.0.local_ln1\n",
      "blocks.0.local_dense\n",
      "blocks.0.local_dense.0\n",
      "blocks.0.local_dense.0.fn\n",
      "blocks.0.local_dense.0.fn.0\n",
      "blocks.0.local_dense.0.fn.1\n",
      "blocks.0.local_dense.1\n",
      "blocks.0.global_dense1\n",
      "blocks.0.global_dense1.0\n",
      "blocks.0.global_dense1.1\n",
      "blocks.0.global_attention\n",
      "blocks.0.global_attention.to_q\n",
      "blocks.0.global_attention.to_q.0\n",
      "blocks.0.global_attention.to_q.1\n",
      "blocks.0.global_attention.to_k\n",
      "blocks.0.global_attention.to_k.0\n",
      "blocks.0.global_attention.to_k.1\n",
      "blocks.0.global_attention.to_v\n",
      "blocks.0.global_attention.to_v.0\n",
      "blocks.0.global_attention.to_v.1\n",
      "blocks.0.global_ln1\n",
      "blocks.0.global_dense2\n",
      "blocks.0.global_dense2.0\n",
      "blocks.0.global_dense2.0.fn\n",
      "blocks.0.global_dense2.0.fn.0\n",
      "blocks.0.global_dense2.0.fn.1\n",
      "blocks.0.global_dense2.1\n",
      "blocks.1\n",
      "blocks.1.wide_and_narrow_conv1d\n",
      "blocks.1.wide_and_narrow_conv1d.conv_narrow\n",
      "blocks.1.wide_and_narrow_conv1d.conv_narrow.0\n",
      "blocks.1.wide_and_narrow_conv1d.conv_narrow.1\n",
      "blocks.1.wide_and_narrow_conv1d.conv_narrow.2\n",
      "blocks.1.wide_and_narrow_conv1d.conv_narrow.3\n",
      "blocks.1.wide_and_narrow_conv1d.conv_wide\n",
      "blocks.1.wide_and_narrow_conv1d.conv_wide.0\n",
      "blocks.1.wide_and_narrow_conv1d.conv_wide.1\n",
      "blocks.1.wide_and_narrow_conv1d.conv_wide.2\n",
      "blocks.1.wide_and_narrow_conv1d.conv_wide.3\n",
      "blocks.1.dense_and_broadcast\n",
      "blocks.1.dense_and_broadcast.0\n",
      "blocks.1.dense_and_broadcast.1\n",
      "blocks.1.dense_and_broadcast.2\n",
      "blocks.1.local_ln1\n",
      "blocks.1.local_dense\n",
      "blocks.1.local_dense.0\n",
      "blocks.1.local_dense.0.fn\n",
      "blocks.1.local_dense.0.fn.0\n",
      "blocks.1.local_dense.0.fn.1\n",
      "blocks.1.local_dense.1\n",
      "blocks.1.global_dense1\n",
      "blocks.1.global_dense1.0\n",
      "blocks.1.global_dense1.1\n",
      "blocks.1.global_attention\n",
      "blocks.1.global_attention.to_q\n",
      "blocks.1.global_attention.to_q.0\n",
      "blocks.1.global_attention.to_q.1\n",
      "blocks.1.global_attention.to_k\n",
      "blocks.1.global_attention.to_k.0\n",
      "blocks.1.global_attention.to_k.1\n",
      "blocks.1.global_attention.to_v\n",
      "blocks.1.global_attention.to_v.0\n",
      "blocks.1.global_attention.to_v.1\n",
      "blocks.1.global_ln1\n",
      "blocks.1.global_dense2\n",
      "blocks.1.global_dense2.0\n",
      "blocks.1.global_dense2.0.fn\n",
      "blocks.1.global_dense2.0.fn.0\n",
      "blocks.1.global_dense2.0.fn.1\n",
      "blocks.1.global_dense2.1\n",
      "blocks.2\n",
      "blocks.2.wide_and_narrow_conv1d\n",
      "blocks.2.wide_and_narrow_conv1d.conv_narrow\n",
      "blocks.2.wide_and_narrow_conv1d.conv_narrow.0\n",
      "blocks.2.wide_and_narrow_conv1d.conv_narrow.1\n",
      "blocks.2.wide_and_narrow_conv1d.conv_narrow.2\n",
      "blocks.2.wide_and_narrow_conv1d.conv_narrow.3\n",
      "blocks.2.wide_and_narrow_conv1d.conv_wide\n",
      "blocks.2.wide_and_narrow_conv1d.conv_wide.0\n",
      "blocks.2.wide_and_narrow_conv1d.conv_wide.1\n",
      "blocks.2.wide_and_narrow_conv1d.conv_wide.2\n",
      "blocks.2.wide_and_narrow_conv1d.conv_wide.3\n",
      "blocks.2.dense_and_broadcast\n",
      "blocks.2.dense_and_broadcast.0\n",
      "blocks.2.dense_and_broadcast.1\n",
      "blocks.2.dense_and_broadcast.2\n",
      "blocks.2.local_ln1\n",
      "blocks.2.local_dense\n",
      "blocks.2.local_dense.0\n",
      "blocks.2.local_dense.0.fn\n",
      "blocks.2.local_dense.0.fn.0\n",
      "blocks.2.local_dense.0.fn.1\n",
      "blocks.2.local_dense.1\n",
      "blocks.2.global_dense1\n",
      "blocks.2.global_dense1.0\n",
      "blocks.2.global_dense1.1\n",
      "blocks.2.global_attention\n",
      "blocks.2.global_attention.to_q\n",
      "blocks.2.global_attention.to_q.0\n",
      "blocks.2.global_attention.to_q.1\n",
      "blocks.2.global_attention.to_k\n",
      "blocks.2.global_attention.to_k.0\n",
      "blocks.2.global_attention.to_k.1\n",
      "blocks.2.global_attention.to_v\n",
      "blocks.2.global_attention.to_v.0\n",
      "blocks.2.global_attention.to_v.1\n",
      "blocks.2.global_ln1\n",
      "blocks.2.global_dense2\n",
      "blocks.2.global_dense2.0\n",
      "blocks.2.global_dense2.0.fn\n",
      "blocks.2.global_dense2.0.fn.0\n",
      "blocks.2.global_dense2.0.fn.1\n",
      "blocks.2.global_dense2.1\n",
      "blocks.3\n",
      "blocks.3.wide_and_narrow_conv1d\n",
      "blocks.3.wide_and_narrow_conv1d.conv_narrow\n",
      "blocks.3.wide_and_narrow_conv1d.conv_narrow.0\n",
      "blocks.3.wide_and_narrow_conv1d.conv_narrow.1\n",
      "blocks.3.wide_and_narrow_conv1d.conv_narrow.2\n",
      "blocks.3.wide_and_narrow_conv1d.conv_narrow.3\n",
      "blocks.3.wide_and_narrow_conv1d.conv_wide\n",
      "blocks.3.wide_and_narrow_conv1d.conv_wide.0\n",
      "blocks.3.wide_and_narrow_conv1d.conv_wide.1\n",
      "blocks.3.wide_and_narrow_conv1d.conv_wide.2\n",
      "blocks.3.wide_and_narrow_conv1d.conv_wide.3\n",
      "blocks.3.dense_and_broadcast\n",
      "blocks.3.dense_and_broadcast.0\n",
      "blocks.3.dense_and_broadcast.1\n",
      "blocks.3.dense_and_broadcast.2\n",
      "blocks.3.local_ln1\n",
      "blocks.3.local_dense\n",
      "blocks.3.local_dense.0\n",
      "blocks.3.local_dense.0.fn\n",
      "blocks.3.local_dense.0.fn.0\n",
      "blocks.3.local_dense.0.fn.1\n",
      "blocks.3.local_dense.1\n",
      "blocks.3.global_dense1\n",
      "blocks.3.global_dense1.0\n",
      "blocks.3.global_dense1.1\n",
      "blocks.3.global_attention\n",
      "blocks.3.global_attention.to_q\n",
      "blocks.3.global_attention.to_q.0\n",
      "blocks.3.global_attention.to_q.1\n",
      "blocks.3.global_attention.to_k\n",
      "blocks.3.global_attention.to_k.0\n",
      "blocks.3.global_attention.to_k.1\n",
      "blocks.3.global_attention.to_v\n",
      "blocks.3.global_attention.to_v.0\n",
      "blocks.3.global_attention.to_v.1\n",
      "blocks.3.global_ln1\n",
      "blocks.3.global_dense2\n",
      "blocks.3.global_dense2.0\n",
      "blocks.3.global_dense2.0.fn\n",
      "blocks.3.global_dense2.0.fn.0\n",
      "blocks.3.global_dense2.0.fn.1\n",
      "blocks.3.global_dense2.1\n",
      "blocks.4\n",
      "blocks.4.wide_and_narrow_conv1d\n",
      "blocks.4.wide_and_narrow_conv1d.conv_narrow\n",
      "blocks.4.wide_and_narrow_conv1d.conv_narrow.0\n",
      "blocks.4.wide_and_narrow_conv1d.conv_narrow.1\n",
      "blocks.4.wide_and_narrow_conv1d.conv_narrow.2\n",
      "blocks.4.wide_and_narrow_conv1d.conv_narrow.3\n",
      "blocks.4.wide_and_narrow_conv1d.conv_wide\n",
      "blocks.4.wide_and_narrow_conv1d.conv_wide.0\n",
      "blocks.4.wide_and_narrow_conv1d.conv_wide.1\n",
      "blocks.4.wide_and_narrow_conv1d.conv_wide.2\n",
      "blocks.4.wide_and_narrow_conv1d.conv_wide.3\n",
      "blocks.4.dense_and_broadcast\n",
      "blocks.4.dense_and_broadcast.0\n",
      "blocks.4.dense_and_broadcast.1\n",
      "blocks.4.dense_and_broadcast.2\n",
      "blocks.4.local_ln1\n",
      "blocks.4.local_dense\n",
      "blocks.4.local_dense.0\n",
      "blocks.4.local_dense.0.fn\n",
      "blocks.4.local_dense.0.fn.0\n",
      "blocks.4.local_dense.0.fn.1\n",
      "blocks.4.local_dense.1\n",
      "blocks.4.global_dense1\n",
      "blocks.4.global_dense1.0\n",
      "blocks.4.global_dense1.1\n",
      "blocks.4.global_attention\n",
      "blocks.4.global_attention.to_q\n",
      "blocks.4.global_attention.to_q.0\n",
      "blocks.4.global_attention.to_q.1\n",
      "blocks.4.global_attention.to_k\n",
      "blocks.4.global_attention.to_k.0\n",
      "blocks.4.global_attention.to_k.1\n",
      "blocks.4.global_attention.to_v\n",
      "blocks.4.global_attention.to_v.0\n",
      "blocks.4.global_attention.to_v.1\n",
      "blocks.4.global_ln1\n",
      "blocks.4.global_dense2\n",
      "blocks.4.global_dense2.0\n",
      "blocks.4.global_dense2.0.fn\n",
      "blocks.4.global_dense2.0.fn.0\n",
      "blocks.4.global_dense2.0.fn.1\n",
      "blocks.4.global_dense2.1\n",
      "blocks.5\n",
      "blocks.5.wide_and_narrow_conv1d\n",
      "blocks.5.wide_and_narrow_conv1d.conv_narrow\n",
      "blocks.5.wide_and_narrow_conv1d.conv_narrow.0\n",
      "blocks.5.wide_and_narrow_conv1d.conv_narrow.1\n",
      "blocks.5.wide_and_narrow_conv1d.conv_narrow.2\n",
      "blocks.5.wide_and_narrow_conv1d.conv_narrow.3\n",
      "blocks.5.wide_and_narrow_conv1d.conv_wide\n",
      "blocks.5.wide_and_narrow_conv1d.conv_wide.0\n",
      "blocks.5.wide_and_narrow_conv1d.conv_wide.1\n",
      "blocks.5.wide_and_narrow_conv1d.conv_wide.2\n",
      "blocks.5.wide_and_narrow_conv1d.conv_wide.3\n",
      "blocks.5.dense_and_broadcast\n",
      "blocks.5.dense_and_broadcast.0\n",
      "blocks.5.dense_and_broadcast.1\n",
      "blocks.5.dense_and_broadcast.2\n",
      "blocks.5.local_ln1\n",
      "blocks.5.local_dense\n",
      "blocks.5.local_dense.0\n",
      "blocks.5.local_dense.0.fn\n",
      "blocks.5.local_dense.0.fn.0\n",
      "blocks.5.local_dense.0.fn.1\n",
      "blocks.5.local_dense.1\n",
      "blocks.5.global_dense1\n",
      "blocks.5.global_dense1.0\n",
      "blocks.5.global_dense1.1\n",
      "blocks.5.global_attention\n",
      "blocks.5.global_attention.to_q\n",
      "blocks.5.global_attention.to_q.0\n",
      "blocks.5.global_attention.to_q.1\n",
      "blocks.5.global_attention.to_k\n",
      "blocks.5.global_attention.to_k.0\n",
      "blocks.5.global_attention.to_k.1\n",
      "blocks.5.global_attention.to_v\n",
      "blocks.5.global_attention.to_v.0\n",
      "blocks.5.global_attention.to_v.1\n",
      "blocks.5.global_ln1\n",
      "blocks.5.global_dense2\n",
      "blocks.5.global_dense2.0\n",
      "blocks.5.global_dense2.0.fn\n",
      "blocks.5.global_dense2.0.fn.0\n",
      "blocks.5.global_dense2.0.fn.1\n",
      "blocks.5.global_dense2.1\n",
      "local_head\n",
      "local_head.0\n",
      "global_head\n",
      "global_head.0\n",
      "global_head.1\n"
     ]
    }
   ],
   "source": [
    "def denoise(targets, timestep, attention_mask, *, model):\n",
    "    #ret = model(targets)\n",
    "    ret = model(targets, attention_mask=attention_mask)\n",
    "    # print(\"denoise output:\", ret.shape)\n",
    "    return ret\n",
    "\n",
    "with open(\"../weights/epoch_92400_sample_23500000.pkl\", \"rb\") as f:\n",
    "    _, pretrained_model_weights, _ = pickle.load(f)\n",
    "\n",
    "model = ProteinBERT(tokenizer.vocab_size, consts.GO_ANN_SIZE)\n",
    "print(model)\n",
    "\n",
    "trainable_params = load_pretrained_weights(model, pretrained_model_weights)\n",
    "model = model.to(args.device)\n",
    "denoise_fn = partial(denoise, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f059ff8-7325-472b-849e-7e61f881f483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/.miniforge3/envs/proteinbert_gen/lib/python3.11/site-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(trainable_params, lr=args.lr)\n",
    "# warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "#     optimizer,\n",
    "#     lr_lambda=lambda n: n / 10000. + 1e-3 if n < 10000 else 100. / math.sqrt(n)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5497e45a-ddc8-4618-8139-3a7b25304847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using standard schedule with num_steps: 2048.\n"
     ]
    }
   ],
   "source": [
    "sample_cls = Categorical()\n",
    "\n",
    "diffusion_schedule = mask_diffusion.create_discrete_diffusion_schedule(num_steps=args.num_steps)\n",
    "diffusion_instance = mask_diffusion.MaskDiffusion(\n",
    "    dim=tokenizer.vocab_size,\n",
    "    schedule=diffusion_schedule,\n",
    "    tokenizer=tokenizer,\n",
    "    sample_cls=sample_cls,\n",
    "    word_freq_lambda=args.word_freq_lambda,\n",
    "    device=args.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410a448-cb86-44f9-be96-17fd4847930a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████████████████████████████▍                             | 3251/5082 [03:35<02:00, 15.24it/s]"
     ]
    }
   ],
   "source": [
    "train_loss = 0.\n",
    "nan_count = 0\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# def _save_output(module, grad_input, grad_output):\n",
    "#     print(module, grad_output)\n",
    "#     print(torch.isnan(grad_output[0]).any())\n",
    "#     print()\n",
    "\n",
    "# for name, module in model.named_modules():\n",
    "#     if str(type(module)).find(\"LayerNorm\") != -1:\n",
    "#         print(name)\n",
    "#         module.register_full_backward_hook(_save_output)\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        diffusion_t = diffusion_instance.sample_t()\n",
    "        # print(diffusion_t)\n",
    "\n",
    "        metrics = mask_diffusion.compute_kl_reverse_process(\n",
    "            batch[\"input_ids\"].to(args.device),\n",
    "            diffusion_t,\n",
    "            denoise_fn=denoise_fn,\n",
    "            diffusion=diffusion_instance,\n",
    "            target_mask=batch[\"attention_mask\"].to(args.device),\n",
    "            hybrid_lambda=args.hybrid_lambda,\n",
    "            predict_x0=True,\n",
    "            word_freq_logits=batch[\"word_freq_logits\"].to(args.device),\n",
    "            device=args.device\n",
    "        )\n",
    "\n",
    "        # print(metrics)\n",
    "\n",
    "        loss = metrics[\"loss\"] / args.batch_size\n",
    "\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        if loss.isnan():\n",
    "            nan_count += 1\n",
    "            continue\n",
    "            \n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(trainable_params, 1)\n",
    "        for param in trainable_params:\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print2(\"setting nan gradients to 0\", tags=[\"info\"])\n",
    "                    param.grad = torch.nan_to_num(param.grad, nan=0.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        # warmup_scheduler.step()\n",
    "        # print(warmup_scheduler.get_last_lr())\n",
    "\n",
    "        # if i % args.logging_steps == args.logging_steps - 1:\n",
    "        #     print(f\"Loss at step {i} is {train_loss / args.logging_steps}\")\n",
    "        #     train_loss = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c65338-bdd4-4978-8d9a-d7e92a069709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def gen_random_seq(length, mask_pct=0.75):\n",
    "    rand_seq = [random.choice(tokenizer.ALL_AMINO_ACIDS) for _ in range(80)]\n",
    "    for i in range(length):\n",
    "        if random.random() < mask_pct:\n",
    "            rand_seq[i] = \"X\"\n",
    "    return \"\".join(rand_seq)\n",
    "\n",
    "rand_seqs = [gen_random_seq(80) for _ in range(32)]\n",
    "noise = [{\"seq\": seq} for seq in rand_seqs]\n",
    "noise = collate_fn(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d091ded-0aa0-4773-b4f1-c095d6fd141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rand_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789a3d6-8442-44e7-838f-826ac3ff2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = model(noise[\"input_ids\"].to(args.device), attention_mask=noise[\"attention_mask\"].to(args.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f894007-3c71-48f4-8120-7adf9461bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_list = generated.argmax(dim=-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b6cb2-525d-41d6-a0bd-a807447b7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ProteinTokenizer()\n",
    "\n",
    "for seq in generated_list:\n",
    "    print(tokenizer.untokenize(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f31f6-aa7e-4d97-bfc5-4aca39c292fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
