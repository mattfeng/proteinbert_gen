{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e5a8ca-9eae-41a7-a1be-cde0d463b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ecf403a-9e4d-48a3-9673-9b35a73c1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW, SGD\n",
    "from collections import namedtuple\n",
    "\n",
    "import proteinbert_gen.constants as consts\n",
    "import proteinbert_gen.mask_diffusion as mask_diffusion\n",
    "\n",
    "from proteinbert_gen.proteinbert import ProteinBERT, load_pretrained_weights\n",
    "from proteinbert_gen.word_freq import create_word_freq_tensor\n",
    "from proteinbert_gen.tokenizer import ProteinTokenizer\n",
    "from proteinbert_gen.dataset import sprot_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe024125-8e53-4eed-817a-baff253eed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hyperparameters = namedtuple(\n",
    "    \"Hyperparameters\",\n",
    "    [\n",
    "        \"batch_size\",\n",
    "        \"epochs\",\n",
    "        \"num_steps\",\n",
    "        \"word_freq_lambda\",\n",
    "        \"device\",\n",
    "        \"hybrid_lambda\",\n",
    "        \"lr\",\n",
    "        \"logging_steps\",\n",
    "        \"eval_step_size\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "args = Hyperparameters(\n",
    "    batch_size=32,\n",
    "    epochs=1,\n",
    "    num_steps=2048,\n",
    "    word_freq_lambda=0.3,\n",
    "    device=\"cpu\",\n",
    "    hybrid_lambda=1e-2,\n",
    "    lr=5e-4,\n",
    "    logging_steps=10,\n",
    "    eval_step_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "178e9acb-8320-4ef8-8f97-907446412932",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleClassBase(abc.ABC):\n",
    "    def sample(self, logits, x_0):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def post_process_sample_in_prediction(self, sample, x_0):\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Categorical(SampleClassBase):\n",
    "    def sample(self, logits, x_0):\n",
    "        return torch.distributions.categorical.Categorical(logits=logits).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae28cd3c-6a7d-4602-802f-bdba23dbc6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9930, 0.8760, 0.9626, 0.9751, 0.9424, 0.9824, 0.9082, 0.9710, 0.9673,\n",
       "        1.0000, 0.9151, 0.9418, 0.9518, 0.9393, 0.9652, 0.9708, 0.9611, 0.3411,\n",
       "        0.9802, 0.8615, 0.5221, 0.9240, 0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_freq_preprocess_fn(wf):\n",
    "    wf = wf + 1\n",
    "    wf = wf.log()\n",
    "    wf = wf / wf.max()\n",
    "\n",
    "    # range: 0 - 1\n",
    "    return wf\n",
    "\n",
    "def process_fn_in_collate(wf):\n",
    "    return wf - wf.mean()\n",
    "\n",
    "\n",
    "tokenizer = ProteinTokenizer()\n",
    "wf_tensor = create_word_freq_tensor(\"../data/sprot_1m_word_freq_dict.pkl\", tokenizer.ALL_TOKENS)\n",
    "wf_tensor = word_freq_preprocess_fn(wf_tensor)\n",
    "wf_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ddc04c6-8aeb-416a-a825-77e930ce4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch_input, *, tokenizer, word_freq: torch.Tensor):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    word_freq_logits = []\n",
    "    \n",
    "    for item in batch_input:\n",
    "        seq = item[\"seq\"]\n",
    "        ids = torch.tensor(tokenizer.tokenize(seq))\n",
    "        mask = torch.ones_like(ids)\n",
    "        logits = process_fn_in_collate(\n",
    "            word_freq.gather(0, ids)\n",
    "        )\n",
    "        \n",
    "        input_ids.append(ids)\n",
    "        attention_mask.append(mask)\n",
    "        word_freq_logits.append(logits)\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True)\n",
    "    word_freq_logits = pad_sequence(word_freq_logits, batch_first=True)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"word_freq_logits\": word_freq_logits\n",
    "    }\n",
    "\n",
    "collate_fn = partial(collate, tokenizer=tokenizer, word_freq=wf_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adec62cd-06a3-4a73-ab53-9ee7919a5e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    sprot_train,\n",
    "    batch_size=args.batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbb10d7d-dfae-468b-8fa4-d015b0ef4d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[23, 10,  0,  ..., 25, 25, 25],\n",
       "         [23, 10, 16,  ..., 25, 25, 25],\n",
       "         [23, 10,  8,  ..., 25, 25, 25],\n",
       "         ...,\n",
       "         [23, 10,  0,  ..., 25, 25, 25],\n",
       "         [23, 10,  0,  ..., 25, 25, 25],\n",
       "         [23, 10,  7,  ..., 25, 25, 25]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'word_freq_logits': tensor([[-0.9607, -0.0456,  0.0322,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.9570, -0.0418,  0.0042,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.9610, -0.0459,  0.0063,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-0.9529, -0.0378,  0.0401,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.9670, -0.0519,  0.0259,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.9460, -0.0309,  0.0250,  ...,  0.0000,  0.0000,  0.0000]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be8b942d-1faa-4bc3-ae1d-421414e5f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(targets, timestep, attention_mask, *, model):\n",
    "    #ret = model(targets)\n",
    "    ret = model(targets, attention_mask=attention_mask)\n",
    "    # print(\"denoise output:\", ret.shape)\n",
    "    return ret\n",
    "\n",
    "with open(\"../weights/epoch_92400_sample_23500000.pkl\", \"rb\") as f:\n",
    "    _, pretrained_model_weights, _ = pickle.load(f)\n",
    "\n",
    "model = ProteinBERT(tokenizer.vocab_size, consts.GO_ANN_SIZE)\n",
    "load_pretrained_weights(model, pretrained_model_weights)\n",
    "denoise_fn = partial(denoise, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f059ff8-7325-472b-849e-7e61f881f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(model.parameters(), lr=args.lr)\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda n: n / 10000. + 1e-3 if n < 10000 else 100. / math.sqrt(n)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5497e45a-ddc8-4618-8139-3a7b25304847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using standard schedule with num_steps: 2048.\n"
     ]
    }
   ],
   "source": [
    "sample_cls = Categorical()\n",
    "\n",
    "diffusion_schedule = mask_diffusion.create_discrete_diffusion_schedule(num_steps=args.num_steps)\n",
    "diffusion_instance = mask_diffusion.MaskDiffusion(\n",
    "    dim=tokenizer.vocab_size,\n",
    "    schedule=diffusion_schedule,\n",
    "    tokenizer=tokenizer,\n",
    "    sample_cls=sample_cls,\n",
    "    word_freq_lambda=args.word_freq_lambda,\n",
    "    device=args.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3410a448-cb86-44f9-be96-17fd4847930a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/10164 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([212.3911], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(182.0491, grad_fn=<SumBackward0>), 'base_loss': tensor([30.3420], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(35355.8125, grad_fn=<SumBackward0>), 't0_loss': tensor([0.], grad_fn=<MulBackward0>), 'kl_loss': tensor(30.3420, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                  | 1/10164 [00:00<2:47:51,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([1370.2168], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(1309.5519, grad_fn=<SumBackward0>), 'base_loss': tensor([60.6649], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(192429.5781, grad_fn=<SumBackward0>), 't0_loss': tensor([0.], grad_fn=<MulBackward0>), 'kl_loss': tensor(60.6649, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                  | 2/10164 [00:01<2:27:18,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([843.9199], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(791.8787, grad_fn=<SumBackward0>), 'base_loss': tensor([52.0413], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(125629.8828, grad_fn=<SumBackward0>), 't0_loss': tensor([0.], grad_fn=<MulBackward0>), 'kl_loss': tensor(52.0413, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                  | 4/10164 [00:02<1:39:31,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                  | 5/10164 [00:03<1:22:36,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                  | 6/10164 [00:03<1:12:15,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                  | 7/10164 [00:03<1:05:53,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                    | 8/10164 [00:03<58:13,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                    | 9/10164 [00:04<55:33,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                   | 10/10164 [00:04<52:24,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                   | 11/10164 [00:04<53:39,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                   | 12/10164 [00:05<54:09,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                   | 13/10164 [00:05<52:25,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                   | 14/10164 [00:05<50:06,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                   | 15/10164 [00:05<47:31,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 16/10164 [00:06<47:43,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 17/10164 [00:06<47:12,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 18/10164 [00:06<48:46,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 19/10164 [00:07<49:00,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 20/10164 [00:07<47:53,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 21/10164 [00:07<48:03,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 22/10164 [00:07<46:42,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 23/10164 [00:08<47:28,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 24/10164 [00:08<48:12,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 25/10164 [00:08<47:53,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 26/10164 [00:09<46:14,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 27/10164 [00:09<48:06,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 28/10164 [00:09<46:31,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 29/10164 [00:09<47:03,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                  | 30/10164 [00:10<48:24,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                  | 31/10164 [00:10<46:21,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                  | 32/10164 [00:10<46:44,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                  | 33/10164 [00:11<48:28,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                  | 34/10164 [00:11<56:41,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor([nan], grad_fn=<AddBackward0>), 'denominator': 1, 'hybrid_loss': tensor(nan, grad_fn=<SumBackward0>), 'base_loss': tensor([nan], grad_fn=<AddBackward0>), 'cross_entropy_loss': tensor(nan, grad_fn=<SumBackward0>), 't0_loss': tensor([nan], grad_fn=<MulBackward0>), 'kl_loss': tensor(nan, grad_fn=<SumBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m diffusion_t \u001b[38;5;241m=\u001b[39m diffusion_instance\u001b[38;5;241m.\u001b[39msample_t()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print(diffusion_t)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmask_diffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_kl_reverse_process\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiffusion_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdenoise_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdenoise_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiffusion_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhybrid_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhybrid_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_x0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mword_freq_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword_freq_logits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics)\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m args\u001b[38;5;241m.\u001b[39mbatch_size\n",
      "File \u001b[0;32m~/proteinbert_gen/proteinbert_gen/mask_diffusion.py:766\u001b[0m, in \u001b[0;36mcompute_kl_reverse_process\u001b[0;34m(x_start, t, diffusion, denoise_fn, predict_x0, log_space, hybrid_lambda, use_cached_transition, target_mask, word_freq_logits, step_size, device)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;66;03m# print(\"q_t\", q_t.shape, q_t)\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;66;03m# print(\"x_t+1\", x_t_plus_1.shape, x_t_plus_1)\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;66;03m# print(\"transition_probs\", transition_probs.shape, transition_probs)\u001b[39;00m\n\u001b[1;32m    764\u001b[0m transition_probs \u001b[38;5;241m=\u001b[39m transition_probs \u001b[38;5;28;01mif\u001b[39;00m use_cached_transition \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 766\u001b[0m p_t \u001b[38;5;241m=\u001b[39m \u001b[43mp_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdenoise_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_t_plus_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_x0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_x0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_x0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_x0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhybrid_lambda\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransition_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransition_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mword_freq_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mword_freq_logits\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predict_x0 \u001b[38;5;129;01mand\u001b[39;00m hybrid_lambda \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    781\u001b[0m     p_t, p_0 \u001b[38;5;241m=\u001b[39m p_t\n",
      "File \u001b[0;32m~/proteinbert_gen/proteinbert_gen/mask_diffusion.py:652\u001b[0m, in \u001b[0;36mp_forward\u001b[0;34m(denoise_fn, target_mask, x_t, t, diffusion, predict_x0, return_x0, return_logits, special_case_x0, transition_probs, transition_probs_in_logits, maximum_likelihood, epsilon, step_size, word_freq_logits)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns probabilities from the reverse process p(x_{t-1} | x_t).\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03mdenoise_fn: the reverse process. Must support embed, call, and attend.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03mis True)\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (step_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predict_x0)\n\u001b[0;32m--> 652\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mdenoise_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)(logits)\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predict_x0:\n",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m, in \u001b[0;36mdenoise\u001b[0;34m(targets, timestep, attention_mask, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdenoise\u001b[39m(targets, timestep, attention_mask, \u001b[38;5;241m*\u001b[39m, model):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#ret = model(targets)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# print(\"denoise output:\", ret.shape)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/.miniforge3/envs/proteinbert_gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniforge3/envs/proteinbert_gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/proteinbert_gen/proteinbert_gen/proteinbert.py:151\u001b[0m, in \u001b[0;36mProteinBERT.forward\u001b[0;34m(self, x_local, x_global, attention_mask)\u001b[0m\n\u001b[1;32m    148\u001b[0m x_global \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_global(x_global)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 151\u001b[0m     x_local, x_global \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_local\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_global\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_head(x_local)\n",
      "File \u001b[0;32m~/.miniforge3/envs/proteinbert_gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniforge3/envs/proteinbert_gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/proteinbert_gen/proteinbert_gen/proteinbert.py:112\u001b[0m, in \u001b[0;36mTransformerLikeBlock.forward\u001b[0;34m(self, x_local, x_global, attention_mask)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_local, x_global, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    109\u001b[0m     x_local \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_ln1(\n\u001b[1;32m    110\u001b[0m         x_local \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwide_and_narrow_conv1d(x_local) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_and_broadcast(x_global)\n\u001b[1;32m    111\u001b[0m     )\n\u001b[0;32m--> 112\u001b[0m     x_local \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_local\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     x_global \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_ln1(\n\u001b[1;32m    115\u001b[0m         x_global \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_dense1(x_global) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_attention(x_local, x_global, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     x_global \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_dense2(x_global)\n",
      "File \u001b[0;32m~/.miniforge3/envs/proteinbert_gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniforge3/envs/proteinbert_gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniforge3/envs/proteinbert_gen/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.miniforge3/envs/proteinbert_gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniforge3/envs/proteinbert_gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/proteinbert_gen/proteinbert_gen/proteinbert.py:17\u001b[0m, in \u001b[0;36mResidual.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m~/.miniforge3/envs/proteinbert_gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniforge3/envs/proteinbert_gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniforge3/envs/proteinbert_gen/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.miniforge3/envs/proteinbert_gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniforge3/envs/proteinbert_gen/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniforge3/envs/proteinbert_gen/lib/python3.11/site-packages/torch/nn/modules/activation.py:682\u001b[0m, in \u001b[0;36mGELU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapproximate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproximate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = 0.\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        diffusion_t = diffusion_instance.sample_t()\n",
    "        # print(diffusion_t)\n",
    "\n",
    "        metrics = mask_diffusion.compute_kl_reverse_process(\n",
    "            batch[\"input_ids\"].to(args.device),\n",
    "            diffusion_t,\n",
    "            denoise_fn=denoise_fn,\n",
    "            diffusion=diffusion_instance,\n",
    "            target_mask=batch[\"attention_mask\"].to(args.device),\n",
    "            hybrid_lambda=args.hybrid_lambda,\n",
    "            predict_x0=True,\n",
    "            word_freq_logits=batch[\"word_freq_logits\"].to(args.device)\n",
    "        )\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "        loss = metrics[\"loss\"] / args.batch_size\n",
    "\n",
    "        if loss.isnan():\n",
    "            continue\n",
    "            \n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(model.parameters(), 1)\n",
    "        \n",
    "        optimizer.step()\n",
    "        warmup_scheduler.step()\n",
    "        # print(warmup_scheduler.get_last_lr())\n",
    "\n",
    "        if i % args.logging_steps == args.logging_steps - 1:\n",
    "            print(f\"Loss at step {i} is {train_loss / args.logging_steps}\")\n",
    "            train_loss = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c65338-bdd4-4978-8d9a-d7e92a069709",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789a3d6-8442-44e7-838f-826ac3ff2eab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
